This file is a merged representation of a subset of the codebase, containing files not matching ignore patterns, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching these patterns are excluded: .jj, .venv
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
cert-generator/
  Dockerfile
  generate_certs.sh
  openssl_ca.cnf
  openssl_server.cnf
litellm_proxy/
  config.yaml
nginx/
  nginx.conf
ollama/
  Dockerfile
  entrypoint.sh
python-client/
  custom_mtls_stream_mock_handler.py
  gemini-scratch.py
  litellm_utils.py
  main.py
  mtls.py
  sample.py
tests/
  test_litellm_sdk_integration.py
  test_ollama_setup.py
.gitignore
custom_handler_instructions.md
docker-compose.yml
imports.py
instructions.md
README.md
repomix.sh
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="cert-generator/Dockerfile">
# cert-generator/Dockerfile
FROM alpine:latest

# Add openssl
RUN apk add --no-cache openssl

WORKDIR /certs

# Copy the script and the new OpenSSL CA config file
COPY generate_certs.sh .
COPY openssl_ca.cnf .
COPY openssl_server.cnf .

RUN chmod +x generate_certs.sh

CMD ["./generate_certs.sh"]
</file>

<file path="cert-generator/generate_certs.sh">
#!/bin/sh
set -e

# Output directory inside the container, mapped from host's ./certs
OUTPUT_DIR="/certs_output"
CONFIG_DIR="/certs" # Directory where OpenSSL config files are copied in Dockerfile

# Check if certificates already exist to avoid overwriting unless forced
if [ -f "$OUTPUT_DIR/ca.crt" ] && [ "$FORCE_REGENERATE" != "true" ]; then
  echo "Certificates already exist in $OUTPUT_DIR. Skipping generation."
  echo "Set FORCE_REGENERATE=true to overwrite."
  exit 0
fi

echo "Generating new certificates in $CONFIG_DIR, will be copied to $OUTPUT_DIR ..."

# --- CA Certificate ---
echo "1. Generating CA private key (ca.key)..."
openssl genrsa -out "$CONFIG_DIR/ca.key" 2048

echo "2. Generating CA certificate (ca.crt) using $CONFIG_DIR/openssl_ca.cnf ..."
openssl req -x509 -new -nodes -key "$CONFIG_DIR/ca.key" \
  -sha256 -days 3650 \
  -subj "/C=XX/ST=Testland/L=TestCity/O=LocalTestOrg/OU=DevOps/CN=LocalTestCA" \
  -config "$CONFIG_DIR/openssl_ca.cnf" \
  -extensions v3_ca \
  -out "$CONFIG_DIR/ca.crt"

# --- Server Certificate ---
echo "3. Generating Server private key (server.key)..."
openssl genrsa -out "$CONFIG_DIR/server.key" 2048

echo "4. Generating Server Certificate Signing Request (server.csr) with SAN using $CONFIG_DIR/openssl_server.cnf ..."
# Pass the config file to req to include extensions (like SAN) in the CSR itself
openssl req -new -key "$CONFIG_DIR/server.key" \
  -subj "/C=XX/ST=Testland/L=TestCity/O=LocalTestOrg/OU=Server/CN=localhost" \
  -config "$CONFIG_DIR/openssl_server.cnf" \
  -reqexts v3_req \
  -out "$CONFIG_DIR/server.csr"

echo "5. Signing the Server CSR with the CA (server.crt)..."
# When signing, explicitly copy extensions from the CSR to the certificate
openssl x509 -req -in "$CONFIG_DIR/server.csr" \
  -CA "$CONFIG_DIR/ca.crt" -CAkey "$CONFIG_DIR/ca.key" \
  -CAcreateserial \
  -days 3600 -sha256 \
  -copy_extensions copyall \
  -out "$CONFIG_DIR/server.crt"

# --- Client Certificate ---
echo "6. Generating Client private key (client.key)..."
openssl genrsa -out "$CONFIG_DIR/client.key" 2048

echo "7. Generating Client Certificate Signing Request (client.csr)..."
# For client certs, SAN is less common unless specific use cases demand it.
# We'll keep it simple for now. CN=TestClient should be sufficient for client auth.
openssl req -new -key "$CONFIG_DIR/client.key" \
  -subj "/C=XX/ST=Testland/L=TestCity/O=LocalTestOrg/OU=Client/CN=TestClient" \
  -out "$CONFIG_DIR/client.csr"

echo "8. Signing the Client CSR with the CA (client.crt)..."
openssl x509 -req -in "$CONFIG_DIR/client.csr" \
  -CA "$CONFIG_DIR/ca.crt" -CAkey "$CONFIG_DIR/ca.key" \
  -CAcreateserial \
  -days 3600 -sha256 \
  -out "$CONFIG_DIR/client.crt"
  # Add extensions for client if needed, e.g., clientAuth in extendedKeyUsage
  # For example: -extfile openssl_client.cnf -extensions v3_client

# Clean up CSR files and serial files from the generation directory
rm "$CONFIG_DIR"/*.csr
rm "$CONFIG_DIR"/*.srl 2>/dev/null || true


echo "Certificates generated successfully in $CONFIG_DIR."

if [ -d "$OUTPUT_DIR" ]; then
  echo "Copying certificates to $OUTPUT_DIR..."
  if [ "$FORCE_REGENERATE" = "true" ]; then
    rm -f "$OUTPUT_DIR"/*.*
  fi
  cp "$CONFIG_DIR/ca.crt" "$CONFIG_DIR/ca.key" \
     "$CONFIG_DIR/client.crt" "$CONFIG_DIR/client.key" \
     "$CONFIG_DIR/server.crt" "$CONFIG_DIR/server.key" \
     "$OUTPUT_DIR/"
  echo "Certificates copied to $OUTPUT_DIR."
else
  echo "Warning: $OUTPUT_DIR directory not found. Certificates remain in the container's $CONFIG_DIR directory."
fi

echo "Certificate generation complete."
</file>

<file path="cert-generator/openssl_ca.cnf">
# cert-generator/openssl_ca.cnf

[ req ]
distinguished_name = req_distinguished_name
x509_extensions    = v3_ca  # The extensions to add to the self-signed cert
prompt             = no     # Don't prompt for DN, use subj from command line

[ req_distinguished_name ]
# Subject details are provided via -subj on the command line
# For example:
# C            = XX
# ST           = Testland
# L            = TestCity
# O            = LocalTestOrg
# OU           = DevOps
# CN           = LocalTestCA

[ v3_ca ]
# Extensions for a typical CA
subjectKeyIdentifier   = hash
authorityKeyIdentifier = keyid:always,issuer
basicConstraints       = critical, CA:TRUE
keyUsage               = critical, digitalSignature, cRLSign, keyCertSign
</file>

<file path="cert-generator/openssl_server.cnf">
# cert-generator/openssl_server.cnf

[ req ]
distinguished_name = req_distinguished_name
req_extensions     = v3_req # Extensions for the CSR
prompt             = no

[ req_distinguished_name ]
# Subject details are provided via -subj on the command line

[ v3_req ]
basicConstraints = CA:FALSE
keyUsage = nonRepudiation, digitalSignature, keyEncipherment
subjectAltName = @alt_names

[ alt_names ]
DNS.1 = localhost
DNS.2 = nginx-mtls-proxy
# Added for internal Docker network access
# If you needed to support IP addresses, you could add:
IP.1 = 127.0.0.1
</file>

<file path="litellm_proxy/config.yaml">
# litellm_proxy/config.yaml
model_list:
  - model_name: ollama-qwen-local # Name for accessing this model via the proxy
    litellm_params:
      model: ollama/qwen3:0.6b # Tells LiteLLM to use Ollama provider + specific model tag
      api_base: http://ollama:11434 # Internal docker network address for Ollama service
      # No api_key needed for local Ollama

litellm_settings:
 general_settings:
   master_key: sk-1234 # Master key for LiteLLM proxy auth (not for Ollama)
</file>

<file path="nginx/nginx.conf">
# nginx/nginx.conf
worker_processes 1;

events {
    worker_connections 1024;
}

http {
    upstream backend {
        # 'litellm-proxy' is the service name in docker-compose.yml
        # Port 4000 is the default LiteLLM proxy port
        server litellm-proxy:4000;
    }

    server {
        listen 443 ssl;
        server_name localhost; # Match the CN used in server cert

        # --- SSL/TLS Server Config ---
        ssl_certificate /etc/nginx/certs/server.crt;
        ssl_certificate_key /etc/nginx/certs/server.key;

        # --- mTLS Client Verification ---
        ssl_client_certificate /etc/nginx/certs/ca.crt; # CA cert to verify client certs
        ssl_verify_client on;                          # Require client cert
        ssl_verify_depth 1;                            # Adjust if using intermediate CAs

        # Optional: Send client cert info to backend (if needed)
        # proxy_set_header X-SSL-Client-Cert $ssl_client_escaped_cert;
        # proxy_set_header X-SSL-Client-Verify $ssl_client_verify;
        # proxy_set_header X-SSL-Client-Subject $ssl_client_s_dn;

        location / {
            proxy_pass http://backend; # Forward to the upstream backend service
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            # WebSocket support for streaming if LiteLLM uses it
            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
            proxy_read_timeout 86400; # Long timeout for streaming
            proxy_buffering off;      # Disable buffering for SSE/WebSockets
        }
    }
}
</file>

<file path="ollama/Dockerfile">
FROM ollama/ollama

COPY entrypoint.sh /entrypoint.sh
RUN chmod +x /entrypoint.sh

ENTRYPOINT ["/entrypoint.sh"]
</file>

<file path="ollama/entrypoint.sh">
#!/bin/sh
set -e # Exit immediately if a command exits with a non-zero status.

LOG_FILE="/tmp/ollama_serve.log"

echo "Ollama entrypoint script started."
echo "Logging ollama serve output to $LOG_FILE"

# Start ollama serve in the background, redirecting its stdout and stderr
ollama serve > "$LOG_FILE" 2>&1 &
pid=$! # Get the process ID of ollama serve

# Function to print last few lines of log and kill ollama
fail_and_exit() {
  echo "Ollama failed. Last lines from $LOG_FILE:"
  tail -n 30 "$LOG_FILE" # Print last 30 lines for debugging
  kill $pid >/dev/null 2>&1 || true # Try to kill the background ollama serve process
  echo "Exiting."
  exit 1
}

# Wait for Ollama server to be responsive using "ollama ps"
echo "Waiting for Ollama server to become responsive..."
count=0
max_count=60 # Wait for max 60 seconds
until ollama ps >/dev/null 2>&1; do
  # Check if the background process is still alive
  if ! ps -p $pid > /dev/null; then
    echo "Ollama serve process (PID: $pid) died unexpectedly."
    fail_and_exit
  fi
  sleep 1
  count=$((count+1))
  if [ "$count" -ge "$max_count" ]; then
    echo "Ollama server failed to become responsive (ollama ps) within $max_count seconds."
    fail_and_exit
  fi
  printf "."
done
echo "" # Newline after dots
echo "Ollama server is responsive."
echo "--- Current content of $LOG_FILE (first 20 lines): ---"
head -n 20 "$LOG_FILE"
echo "-------------------------------------------------------"


# Pull the specified model
MODEL_TO_PULL=${OLLAMA_PULL_MODEL:-qwen3:0.6b}
echo "Pulling model: $MODEL_TO_PULL ..."
if ! ollama pull "$MODEL_TO_PULL"; then
    echo "Failed to pull model $MODEL_TO_PULL."
    echo "--- Content of $LOG_FILE on pull failure: ---"
    cat "$LOG_FILE"
    echo "----------------------------------------------"
    kill $pid >/dev/null 2>&1 || true
    exit 1
fi
echo "Model $MODEL_TO_PULL pulled successfully or already exists."

echo "Ollama is running with model $MODEL_TO_PULL. PID: $pid"
wait $pid

echo "Ollama serve process (PID: $pid) has exited."
echo "--- Final content of $LOG_FILE: ---"
cat "$LOG_FILE"
echo "----------------------------------"
exit 0
</file>

<file path="python-client/custom_mtls_stream_mock_handler.py">
import asyncio
import ssl
import time
import uuid
import os
import traceback
from pathlib import Path
from typing import AsyncIterator, Iterator

import httpx
import openai  # For OpenAI client and types
from openai.types.chat import ChatCompletion, ChatCompletionMessage  # For mocking/typing upstream response

import litellm
# Corrected imports for LiteLLM types
from litellm import CustomLLM
from litellm.utils import ModelResponse, Choices, Message, Usage  # LiteLLM specific types
from litellm.types.utils import Delta, StreamingChoices, GenericStreamingChunk  # Import GenericStreamingChunk

from dotenv import load_dotenv

# Assuming litellm_utils.py is in the same directory
try:
    from litellm_utils import convert_openai_chat_completion_to_litellm_model_response
except ImportError:
    print("Error: litellm_utils.py not found. Please ensure it's in the same directory.")
    print("It should contain the function: convert_openai_chat_completion_to_litellm_model_response")
    exit(1)

# --- Certificate Paths ---
current_file_dir = Path(__file__).parent
certs_dir = current_file_dir.parent / "certs"
CERTIFICATE_PATH = certs_dir / "client.crt"
KEY_PATH = certs_dir / "client.key"
CA_PATH = certs_dir / "ca.crt"

# --- Validate Certificate Paths ---
for path_to_check in (CERTIFICATE_PATH, KEY_PATH, CA_PATH):
    if not path_to_check.exists():
        print(f"Error: Could not find certificate file {path_to_check}")
        print(f"Current file directory: {current_file_dir}")
        print(f"Calculated certs directory: {certs_dir}")
        exit(1)
    else:
        print(f"Found certificate file: {path_to_check}")

# --- Load Environment Variables ---
load_dotenv()

# --- Configuration ---
UPSTREAM_MODEL_NAME = "ollama-qwen-local"
LITELLM_PROXY_KEY = "sk-1234"
MTLS_PROXY_URL = "https://localhost:8443"


class MTLSOpenAILLM(CustomLLM):
    """
    Custom LiteLLM handler that:
    1. Uses mTLS to communicate with an upstream OpenAI-compatible API (e.g., LiteLLM Proxy).
    2. Implements "fake streaming" by making a non-streaming call and returning the response
       as a single content chunk followed by a finalization chunk, using GenericStreamingChunk.
    """

    def _get_client(self, asynchronous: bool = False):
        try:
            ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH, cafile=str(CA_PATH))
            ctx.load_cert_chain(certfile=str(CERTIFICATE_PATH), keyfile=str(KEY_PATH))
        except Exception as e:
            raise RuntimeError(f"Failed to create SSL context: {e}")

        timeout_config = httpx.Timeout(600.0, connect=10.0)

        if asynchronous:
            httpx_client = httpx.AsyncClient(verify=ctx, timeout=timeout_config)
            return openai.AsyncOpenAI(
                http_client=httpx_client,
                base_url=MTLS_PROXY_URL,
                api_key=LITELLM_PROXY_KEY
            )
        else:
            httpx_client = httpx.Client(verify=ctx, timeout=timeout_config)
            return openai.OpenAI(
                http_client=httpx_client,
                base_url=MTLS_PROXY_URL,
                api_key=LITELLM_PROXY_KEY
            )

    def completion(self, *args, **kwargs) -> litellm.ModelResponse:
        messages = kwargs.get("messages")
        local_kwargs = kwargs.copy()
        local_kwargs.pop("stream", None)

        sync_openai_client = self._get_client(asynchronous=False)
        try:
            upstream_response: ChatCompletion = sync_openai_client.chat.completions.create(
                model=UPSTREAM_MODEL_NAME,
                messages=messages,
                max_tokens=local_kwargs.get("max_tokens", 150),
                temperature=local_kwargs.get("temperature", 0.7),
            )
        except httpx.ReadTimeout as e:
            raise litellm.Timeout(message=f"MTLS Upstream ReadTimeout: {str(e)}", model=UPSTREAM_MODEL_NAME,
                                  llm_provider="custom_mtls_provider")
        except Exception as e:
            raise litellm.APIConnectionError(f"MTLS Upstream Error: {str(e)}", model=UPSTREAM_MODEL_NAME,
                                             llm_provider="custom_mtls_provider")

        litellm_response = convert_openai_chat_completion_to_litellm_model_response(
            openai_response=upstream_response
        )

        if not hasattr(litellm_response, 'usage') or litellm_response.usage is None:
            prompt_tokens = 0
            completion_tokens = 0
            if messages:
                try:  # Wrap token_counter in try-except as it can sometimes fail with custom/uncommon models
                    prompt_tokens = litellm.token_counter(model=UPSTREAM_MODEL_NAME, messages=messages)
                except:
                    prompt_tokens = 0  # Fallback
            if litellm_response.choices and litellm_response.choices[0].message and litellm_response.choices[
                0].message.content:
                try:
                    completion_tokens = litellm.token_counter(model=UPSTREAM_MODEL_NAME,
                                                              text=litellm_response.choices[0].message.content)
                except:
                    completion_tokens = 0  # Fallback
            litellm_response.usage = Usage(prompt_tokens=prompt_tokens, completion_tokens=completion_tokens,
                                           total_tokens=prompt_tokens + completion_tokens)

        return litellm_response

    async def acompletion(self, *args, **kwargs) -> litellm.ModelResponse:
        messages = kwargs.get("messages")
        local_kwargs = kwargs.copy()
        local_kwargs.pop("stream", None)

        async_openai_client = self._get_client(asynchronous=True)
        try:
            upstream_response: ChatCompletion = await async_openai_client.chat.completions.create(
                model=UPSTREAM_MODEL_NAME,
                messages=messages,
                max_tokens=local_kwargs.get("max_tokens", 150),
                temperature=local_kwargs.get("temperature", 0.7),
            )
        except httpx.ReadTimeout as e:
            raise litellm.Timeout(message=f"MTLS Upstream ReadTimeout: {str(e)}", model=UPSTREAM_MODEL_NAME,
                                  llm_provider="custom_mtls_provider")
        except Exception as e:
            raise litellm.APIConnectionError(f"MTLS Upstream Error: {str(e)}", model=UPSTREAM_MODEL_NAME,
                                             llm_provider="custom_mtls_provider")

        litellm_response = convert_openai_chat_completion_to_litellm_model_response(
            openai_response=upstream_response
        )
        if not hasattr(litellm_response, 'usage') or litellm_response.usage is None:
            prompt_tokens = 0
            completion_tokens = 0
            if messages:
                try:
                    prompt_tokens = litellm.token_counter(model=UPSTREAM_MODEL_NAME, messages=messages)
                except:
                    prompt_tokens = 0
            if litellm_response.choices and litellm_response.choices[0].message and litellm_response.choices[
                0].message.content:
                try:
                    completion_tokens = litellm.token_counter(model=UPSTREAM_MODEL_NAME,
                                                              text=litellm_response.choices[0].message.content)
                except:
                    completion_tokens = 0
            litellm_response.usage = Usage(prompt_tokens=prompt_tokens, completion_tokens=completion_tokens,
                                           total_tokens=prompt_tokens + completion_tokens)

        return litellm_response

    def streaming(self, *args, **kwargs) -> Iterator[GenericStreamingChunk]:
        model_response: ModelResponse = self.completion(*args, **kwargs)

        response_content = ""
        if model_response.choices and model_response.choices[0].message and model_response.choices[0].message.content:
            response_content = model_response.choices[0].message.content

        _model_name = model_response.model or kwargs.get("model", "mtls_custom_model")
        _id = model_response.id or f"chatcmpl-fakestream-sync-{uuid.uuid4()}"
        _created = model_response.created or int(time.time())
        _system_fingerprint = getattr(model_response, 'system_fingerprint', None)

        yield GenericStreamingChunk(
            id=_id,
            object="chat.completion.chunk",
            created=_created,
            model=_model_name,
            system_fingerprint=_system_fingerprint,
            text=response_content,
            is_finished=False,
            finish_reason=None,
            usage=None,
            index=0
        )
        # add debugging
        # print(dict(model_response.usage))
        yield GenericStreamingChunk(
            id=_id,
            object="chat.completion.chunk",
            created=_created,
            model=_model_name,
            system_fingerprint=_system_fingerprint,
            text="",
            is_finished=True,
            finish_reason="stop",
            usage=dict(model_response.usage),
            index=0
        )

    async def astreaming(self, *args, **kwargs) -> AsyncIterator[GenericStreamingChunk]:
        model_response: ModelResponse = await self.acompletion(*args, **kwargs)
        response_content = ""
        if model_response.choices and model_response.choices[0].message and model_response.choices[0].message.content:
            response_content = model_response.choices[0].message.content

        _model_name = model_response.model or kwargs.get("model", "mtls_custom_model")
        _id = model_response.id or f"chatcmpl-fakestream-async-{uuid.uuid4()}"
        _created = model_response.created or int(time.time())
        _system_fingerprint = getattr(model_response, 'system_fingerprint', None)

        yield GenericStreamingChunk(
            id=_id,
            object="chat.completion.chunk",
            created=_created,
            model=_model_name,
            system_fingerprint=_system_fingerprint,
            text=response_content,
            is_finished=False,
            finish_reason=None,
            usage=None,
            index=0
        )
        #add debugging
        # print(model_response)
        yield GenericStreamingChunk(
            id=_id,
            object="chat.completion.chunk",
            created=_created,
            model=_model_name,
            system_fingerprint=_system_fingerprint,
            text="",
            is_finished=True,
            finish_reason="stop",
            usage=dict(model_response.usage),
            index=0
        )


# --- Register the custom LLM provider with LiteLLM ---
my_mtls_openai_llm = MTLSOpenAILLM()

litellm.custom_provider_map = [
    {"provider": "mtls_openai_llm", "custom_handler": my_mtls_openai_llm}
]

# --- Main execution block for testing ---
if __name__ == "__main__":
    # litellm.set_verbose = True

    print("Testing synchronous non-streaming:")
    try:
        resp_sync_non_stream = litellm.completion(
            model="mtls_openai_llm/sync-non-stream",
            messages=[{"role": "user", "content": "Hello from sync non-stream!"}],
        )
        print(f"Response: {resp_sync_non_stream.choices[0].message.content}")
        print(f"Usage: {resp_sync_non_stream.usage}")
    except Exception as e:
        print(f"Error in sync non-streaming: {e}")
        traceback.print_exc()
    print("=" * 40)

    print("Testing asynchronous non-streaming:")
    try:
        resp_async_non_stream = asyncio.run(
            litellm.acompletion(
                model="mtls_openai_llm/async-non-stream",
                messages=[{"role": "user", "content": "Hello from async non-stream!"}],
            )
        )
        print(f"Response: {resp_async_non_stream.choices[0].message.content}")
        print(f"Usage: {resp_async_non_stream.usage}")
    except Exception as e:
        print(f"Error in async non-streaming: {e}")
        traceback.print_exc()
    print("=" * 40)

    print("Testing synchronous streaming:")
    try:
        response_sync_stream = litellm.completion(
            model="mtls_openai_llm/sync-stream",
            messages=[{"role": "user", "content": "Hello from sync stream!"}],
            stream=True, max_tokens=5, stream_options={"include_usage": True}
        )
        full_response_content_sync = ""
        usage_sync = None
        print("Iterating over sync stream:")
        for chunk_num, chunk in enumerate(response_sync_stream):
            content = None
            # For ModelResponseStream, content is in choices[0].delta.content
            if chunk.choices and chunk.choices[0].delta:
                content = chunk.choices[0].delta.content

            finish_reason = chunk.choices[0].finish_reason
            print(f"  Sync Stream Chunk {chunk_num + 1}: finish_reason='{finish_reason}', content='{content or ''}'")
            if content:
                full_response_content_sync += content

            # Usage is typically on the last chunk or when finish_reason is not None
            if hasattr(chunk, 'usage') and chunk.usage is not None:
                usage_sync = chunk.usage
        print(f"Full Sync Streamed Response: {full_response_content_sync}")
        if usage_sync:
            print(f"Sync Stream Usage: {usage_sync}")
    except Exception as e:
        print(f"Error in sync streaming: {e}")
        traceback.print_exc()
    print("=" * 40)

    print("Testing asynchronous streaming:")


    async def consume_async_stream():
        try:
            response_async_stream = await litellm.acompletion(
                model="mtls_openai_llm/async-stream",
                messages=[{"role": "user", "content": "Hello from async stream!"}],
                stream=True,
            )
            full_response_content_async = ""
            usage_async = None
            print("Iterating over async stream:")
            chunk_num = 0
            async for chunk in response_async_stream:
                chunk_num += 1
                content = None
                # For ModelResponseStream, content is in choices[0].delta.content
                if chunk.choices and chunk.choices[0].delta:
                    content = chunk.choices[0].delta.content

                finish_reason = chunk.choices[0].finish_reason
                print(f"  Async Stream Chunk {chunk_num}: finish_reason='{finish_reason}', content='{content or ''}'")
                if content:
                    full_response_content_async += content

                if hasattr(chunk, 'usage') and chunk.usage is not None:
                    usage_async = chunk.usage
            print(f"Full Async Streamed Response: {full_response_content_async}")
            if usage_async:
                print(f"Async Stream Usage: {usage_async}")
        except Exception as e:
            print(f"Error in async streaming: {e}")
            traceback.print_exc()


    asyncio.run(consume_async_stream())
    print("=" * 40)
</file>

<file path="python-client/gemini-scratch.py">
import asyncio
import ssl
import time
import uuid
import os
import traceback
from pathlib import Path
from typing import AsyncIterator, Iterator

import httpx
import openai  # For OpenAI client and types
from openai.types.chat import ChatCompletion, ChatCompletionMessage  # For mocking/typing upstream response

import litellm
# Corrected imports for LiteLLM types
from litellm import CustomLLM
from litellm.utils import ModelResponse, Choices, Message, Usage  # LiteLLM specific types
from litellm.types.utils import Delta, StreamingChoices, GenericStreamingChunk  # Import GenericStreamingChunk

from dotenv import load_dotenv

# Assuming litellm_utils.py is in the same directory
try:
    from litellm_utils import convert_openai_chat_completion_to_litellm_model_response
except ImportError:
    print("Error: litellm_utils.py not found. Please ensure it's in the same directory.")
    print("It should contain the function: convert_openai_chat_completion_to_litellm_model_response")
    exit(1)

# --- Certificate Paths ---
current_file_dir = Path(__file__).parent
certs_dir = current_file_dir.parent / "certs"
CERTIFICATE_PATH = certs_dir / "client.crt"
KEY_PATH = certs_dir / "client.key"
CA_PATH = certs_dir / "ca.crt"

# --- Validate Certificate Paths ---
for path_to_check in (CERTIFICATE_PATH, KEY_PATH, CA_PATH):
    if not path_to_check.exists():
        print(f"Error: Could not find certificate file {path_to_check}")
        print(f"Current file directory: {current_file_dir}")
        print(f"Calculated certs directory: {certs_dir}")
        exit(1)
    else:
        print(f"Found certificate file: {path_to_check}")

# --- Load Environment Variables ---
load_dotenv()

# --- Configuration ---
UPSTREAM_MODEL_NAME = "ollama-qwen-local"
LITELLM_PROXY_KEY = "sk-1234"
MTLS_PROXY_URL = "https://localhost:8443"


class MTLSOpenAILLM(CustomLLM):
    """
    Custom LiteLLM handler that:
    1. Uses mTLS to communicate with an upstream OpenAI-compatible API (e.g., LiteLLM Proxy).
    2. Implements "fake streaming" by making a non-streaming call and returning the response
       as a single content chunk followed by a finalization chunk, using GenericStreamingChunk.
    """

    def _get_client(self, asynchronous: bool = False):
        try:
            ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH, cafile=str(CA_PATH))
            ctx.load_cert_chain(certfile=str(CERTIFICATE_PATH), keyfile=str(KEY_PATH))
        except Exception as e:
            raise RuntimeError(f"Failed to create SSL context: {e}")

        timeout_config = httpx.Timeout(600.0, connect=10.0)

        if asynchronous:
            httpx_client = httpx.AsyncClient(verify=ctx, timeout=timeout_config)
            return openai.AsyncOpenAI(
                http_client=httpx_client,
                base_url=MTLS_PROXY_URL,
                api_key=LITELLM_PROXY_KEY
            )
        else:
            httpx_client = httpx.Client(verify=ctx, timeout=timeout_config)
            return openai.OpenAI(
                http_client=httpx_client,
                base_url=MTLS_PROXY_URL,
                api_key=LITELLM_PROXY_KEY
            )

    def completion(self, *args, **kwargs) -> litellm.ModelResponse:
        messages = kwargs.get("messages")
        local_kwargs = kwargs.copy()
        local_kwargs.pop("stream", None)

        sync_openai_client = self._get_client(asynchronous=False)
        try:
            upstream_response: ChatCompletion = sync_openai_client.chat.completions.create(
                model=UPSTREAM_MODEL_NAME,
                messages=messages,
                max_tokens=local_kwargs.get("max_tokens", 150),
                temperature=local_kwargs.get("temperature", 0.7),
            )
        except httpx.ReadTimeout as e:
            raise litellm.Timeout(message=f"MTLS Upstream ReadTimeout: {str(e)}", model=UPSTREAM_MODEL_NAME,
                                  llm_provider="custom_mtls_provider")
        except Exception as e:
            raise litellm.APIConnectionError(f"MTLS Upstream Error: {str(e)}", model=UPSTREAM_MODEL_NAME,
                                             llm_provider="custom_mtls_provider")

        litellm_response = convert_openai_chat_completion_to_litellm_model_response(
            openai_response=upstream_response
        )

        if not hasattr(litellm_response, 'usage') or litellm_response.usage is None:
            prompt_tokens = 0
            completion_tokens = 0
            if messages:
                try:  # Wrap token_counter in try-except as it can sometimes fail with custom/uncommon models
                    prompt_tokens = litellm.token_counter(model=UPSTREAM_MODEL_NAME, messages=messages)
                except:
                    prompt_tokens = 0  # Fallback
            if litellm_response.choices and litellm_response.choices[0].message and litellm_response.choices[
                0].message.content:
                try:
                    completion_tokens = litellm.token_counter(model=UPSTREAM_MODEL_NAME,
                                                              text=litellm_response.choices[0].message.content)
                except:
                    completion_tokens = 0  # Fallback
            litellm_response.usage = Usage(prompt_tokens=prompt_tokens, completion_tokens=completion_tokens,
                                           total_tokens=prompt_tokens + completion_tokens)

        return litellm_response

    async def acompletion(self, *args, **kwargs) -> litellm.ModelResponse:
        messages = kwargs.get("messages")
        local_kwargs = kwargs.copy()
        local_kwargs.pop("stream", None)

        async_openai_client = self._get_client(asynchronous=True)
        try:
            upstream_response: ChatCompletion = await async_openai_client.chat.completions.create(
                model=UPSTREAM_MODEL_NAME,
                messages=messages,
                max_tokens=local_kwargs.get("max_tokens", 150),
                temperature=local_kwargs.get("temperature", 0.7),
            )
        except httpx.ReadTimeout as e:
            raise litellm.Timeout(message=f"MTLS Upstream ReadTimeout: {str(e)}", model=UPSTREAM_MODEL_NAME,
                                  llm_provider="custom_mtls_provider")
        except Exception as e:
            raise litellm.APIConnectionError(f"MTLS Upstream Error: {str(e)}", model=UPSTREAM_MODEL_NAME,
                                             llm_provider="custom_mtls_provider")

        litellm_response = convert_openai_chat_completion_to_litellm_model_response(
            openai_response=upstream_response
        )
        if not hasattr(litellm_response, 'usage') or litellm_response.usage is None:
            prompt_tokens = 0
            completion_tokens = 0
            if messages:
                try:
                    prompt_tokens = litellm.token_counter(model=UPSTREAM_MODEL_NAME, messages=messages)
                except:
                    prompt_tokens = 0
            if litellm_response.choices and litellm_response.choices[0].message and litellm_response.choices[
                0].message.content:
                try:
                    completion_tokens = litellm.token_counter(model=UPSTREAM_MODEL_NAME,
                                                              text=litellm_response.choices[0].message.content)
                except:
                    completion_tokens = 0
            litellm_response.usage = Usage(prompt_tokens=prompt_tokens, completion_tokens=completion_tokens,
                                           total_tokens=prompt_tokens + completion_tokens)

        return litellm_response

    def streaming(self, *args, **kwargs) -> Iterator[GenericStreamingChunk]:
        model_response: ModelResponse = self.completion(*args, **kwargs)

        response_content = ""
        if model_response.choices and model_response.choices[0].message and model_response.choices[0].message.content:
            response_content = model_response.choices[0].message.content

        _model_name = model_response.model or kwargs.get("model", "mtls_custom_model")
        _id = model_response.id or f"chatcmpl-fakestream-sync-{uuid.uuid4()}"
        _created = model_response.created or int(time.time())
        _system_fingerprint = getattr(model_response, 'system_fingerprint', None)

        yield GenericStreamingChunk(
            id=_id,
            object="chat.completion.chunk",
            created=_created,
            model=_model_name,
            system_fingerprint=_system_fingerprint,
            text=response_content,
            is_finished=False,
            finish_reason=None,
            usage=None,
            index=0
        )
        # add debugging
        print(model_response)
        yield GenericStreamingChunk(
            id=_id,
            object="chat.completion.chunk",
            created=_created,
            model=_model_name,
            system_fingerprint=_system_fingerprint,
            text="",
            is_finished=True,
            finish_reason="stop",
            usage=dict(model_response.usage),
            index=0
        )

    async def astreaming(self, *args, **kwargs) -> AsyncIterator[GenericStreamingChunk]:
        model_response: ModelResponse = await self.acompletion(*args, **kwargs)
        response_content = ""
        if model_response.choices and model_response.choices[0].message and model_response.choices[0].message.content:
            response_content = model_response.choices[0].message.content

        _model_name = model_response.model or kwargs.get("model", "mtls_custom_model")
        _id = model_response.id or f"chatcmpl-fakestream-async-{uuid.uuid4()}"
        _created = model_response.created or int(time.time())
        _system_fingerprint = getattr(model_response, 'system_fingerprint', None)

        yield GenericStreamingChunk(
            id=_id,
            object="chat.completion.chunk",
            created=_created,
            model=_model_name,
            system_fingerprint=_system_fingerprint,
            text=response_content,
            is_finished=False,
            finish_reason=None,
            usage=None,
            index=0
        )
        #add debugging
        print(model_response)
        yield GenericStreamingChunk(
            id=_id,
            object="chat.completion.chunk",
            created=_created,
            model=_model_name,
            system_fingerprint=_system_fingerprint,
            text="",
            is_finished=True,
            finish_reason="stop",
            usage=model_response.usage,
            index=0
        )


# --- Register the custom LLM provider with LiteLLM ---
my_mtls_openai_llm = MTLSOpenAILLM()

litellm.custom_provider_map = [
    {"provider": "mtls_openai_llm", "custom_handler": my_mtls_openai_llm}
]

# --- Main execution block for testing ---
if __name__ == "__main__":
    # litellm.set_verbose = True

    # print("Testing synchronous non-streaming:")
    # try:
    #     resp_sync_non_stream = litellm.completion(
    #         model="mtls_openai_llm/sync-non-stream",
    #         messages=[{"role": "user", "content": "Hello from sync non-stream!"}],
    #     )
    #     print(f"Response: {resp_sync_non_stream.choices[0].message.content}")
    #     print(f"Usage: {resp_sync_non_stream.usage}")
    # except Exception as e:
    #     print(f"Error in sync non-streaming: {e}")
    #     traceback.print_exc()
    # print("=" * 40)
    #
    # print("Testing asynchronous non-streaming:")
    # try:
    #     resp_async_non_stream = asyncio.run(
    #         litellm.acompletion(
    #             model="mtls_openai_llm/async-non-stream",
    #             messages=[{"role": "user", "content": "Hello from async non-stream!"}],
    #         )
    #     )
    #     print(f"Response: {resp_async_non_stream.choices[0].message.content}")
    #     print(f"Usage: {resp_async_non_stream.usage}")
    # except Exception as e:
    #     print(f"Error in async non-streaming: {e}")
    #     traceback.print_exc()
    # print("=" * 40)

    print("Testing synchronous streaming:")
    try:
        response_sync_stream = litellm.completion(
            model="mtls_openai_llm/sync-stream",
            messages=[{"role": "user", "content": "Hello from sync stream!"}],
            stream=True,
        )
        full_response_content_sync = ""
        usage_sync = None
        print("Iterating over sync stream:")
        for chunk_num, chunk in enumerate(response_sync_stream):
            #debug
            print(chunk)
            print(dict(chunk))
            content = None
            # For ModelResponseStream, content is in choices[0].delta.content
            if chunk.choices and chunk.choices[0].delta:
                content = chunk.choices[0].delta.content

            finish_reason = chunk.choices[0].finish_reason
            print(f"  Sync Stream Chunk {chunk_num + 1}: finish_reason='{finish_reason}', content='{content or ''}'")
            if content:
                full_response_content_sync += content

            # Usage is typically on the last chunk or when finish_reason is not None
            if hasattr(chunk, 'usage') and chunk.usage is not None:
                usage_sync = chunk.usage
        print(f"Full Sync Streamed Response: {full_response_content_sync}")
        if usage_sync:
            print(f"Sync Stream Usage: {usage_sync}")
    except Exception as e:
        print(f"Error in sync streaming: {e}")
        traceback.print_exc()
    print("=" * 40)

    print("Testing asynchronous streaming:")


    # async def consume_async_stream():
    #     try:
    #         response_async_stream = await litellm.acompletion(
    #             model="mtls_openai_llm/async-stream",
    #             messages=[{"role": "user", "content": "Hello from async stream!"}],
    #             stream=True,
    #         )
    #         full_response_content_async = ""
    #         usage_async = None
    #         print("Iterating over async stream:")
    #         chunk_num = 0
    #         async for chunk in response_async_stream:
    #             chunk_num += 1
    #             content = None
    #             # For ModelResponseStream, content is in choices[0].delta.content
    #             if chunk.choices and chunk.choices[0].delta:
    #                 content = chunk.choices[0].delta.content
    #
    #             finish_reason = chunk.choices[0].finish_reason
    #             print(f"  Async Stream Chunk {chunk_num}: finish_reason='{finish_reason}', content='{content or ''}'")
    #             if content:
    #                 full_response_content_async += content
    #
    #             if hasattr(chunk, 'usage') and chunk.usage is not None:
    #                 usage_async = chunk.usage
    #         print(f"Full Async Streamed Response: {full_response_content_async}")
    #         if usage_async:
    #             print(f"Async Stream Usage: {usage_async}")
    #     except Exception as e:
    #         print(f"Error in async streaming: {e}")
    #         traceback.print_exc()
    #
    #
    # asyncio.run(consume_async_stream())
    print("=" * 40)
</file>

<file path="python-client/litellm_utils.py">
import time
from typing import Optional, Dict, Any, List, Union  # Standard typing imports
from pydantic import BaseModel
from openai.types.chat import ChatCompletion  # For type hinting the input OpenAI response


# Note: The following Pydantic models (ModelResponseBase, Choices, StreamingChoices, Usage, ModelResponse)
# are defined as per your provided structure. The conversion function below will use these
# locally defined models.

class ModelResponseBase(BaseModel):
    """
    A base Pydantic model that allows extra fields, which is useful for
    handling various parameters that might come from LLM responses.
    """

    class Config:
        extra = 'allow'  # Allow any additional fields not explicitly defined


class Message(BaseModel):  # Added for completeness within Choices
    """
    Represents a message object within a choice, typically containing role and content.
    This is a simplified version; LiteLLM's actual Message can be more complex.
    """
    role: Optional[str] = None
    content: Optional[str] = None
    tool_calls: Optional[List[Any]] = None  # Placeholder for tool_calls structure
    function_call: Optional[Dict[str, Any]] = None  # Placeholder for function_call structure

    class Config:
        extra = 'allow'


class Choices(BaseModel):
    """
    Represents a single choice in a non-streaming LLM response.
    It includes the reason for finishing, the index of the choice, and the message.
    """
    finish_reason: Optional[str] = None
    index: Optional[int] = None
    message: Optional[Message] = None  # Using the Message model defined above

    class Config:
        extra = 'allow'


class Delta(BaseModel):  # Added for completeness within StreamingChoices
    """
    Represents the delta (change) in content for a streaming choice.
    """
    role: Optional[str] = None
    content: Optional[str] = None
    tool_calls: Optional[List[Any]] = None
    function_call: Optional[Dict[str, Any]] = None

    class Config:
        extra = 'allow'


class StreamingChoices(BaseModel):
    """
    Represents a single choice in a streaming LLM response chunk.
    It includes the delta (the new part of the message), finish reason, and index.
    """
    delta: Optional[Delta] = None  # Using the Delta model defined above
    finish_reason: Optional[str] = None
    index: Optional[int] = None
    logprobs: Optional[Any] = None  # Added, as it's common in OpenAI streaming

    class Config:
        extra = 'allow'


class Usage(BaseModel):
    """
    Represents token usage statistics for an LLM request (prompt, completion, total).
    """
    prompt_tokens: Optional[int] = 0
    completion_tokens: Optional[int] = 0
    total_tokens: Optional[int] = 0

    class Config:
        extra = 'allow'


def _generate_id(prefix: str = "chatcmpl-") -> str:
    """
    Generates a unique ID, typically for chat completions, using a timestamp.
    Args:
        prefix: The prefix for the generated ID.
    Returns:
        A string representing the unique ID.
    """
    return f"{prefix}{time.time()}-{hash(time.time())}"  # Added hash for more uniqueness


class ModelResponse(ModelResponseBase):
    """
    Represents a comprehensive LLM response, adaptable for both streaming and non-streaming.
    This class is defined locally as per the user's provided structure.
    """
    id: Optional[str] = None
    choices: List[Union[Choices, StreamingChoices]]
    created: Optional[int] = None
    model: Optional[str] = None
    object: Optional[str] = None
    system_fingerprint: Optional[str] = None
    usage: Optional[Usage] = None
    stream: Optional[bool] = None  # Indicates if this is part of a stream
    stream_options: Optional[Dict[str, Any]] = None
    response_ms: Optional[int] = None
    _hidden_params: Optional[Dict[str, Any]] = None  # For internal LiteLLM use
    _response_headers: Optional[Dict[str, Any]] = None  # For storing response headers

    def __init__(
            self,
            id: Optional[str] = None,
            choices: Optional[List[Union[Dict[str, Any], Choices, StreamingChoices]]] = None,
            created: Optional[int] = None,
            model: Optional[str] = None,
            object: Optional[str] = None,  # e.g., "chat.completion" or "chat.completion.chunk"
            system_fingerprint: Optional[str] = None,
            usage: Optional[Union[Dict[str, Any], Usage]] = None,
            stream: Optional[bool] = None,
            stream_options: Optional[Dict[str, Any]] = None,
            response_ms: Optional[int] = None,
            hidden_params: Optional[Dict[str, Any]] = None,  # Maps to _hidden_params
            _response_headers: Optional[Dict[str, Any]] = None,
            **params,  # Allows for additional fields from the response
    ) -> None:

        actual_choices_list: List[Union[Choices, StreamingChoices]] = []
        if stream is True:
            # Determine object type for streaming responses
            effective_object = object if object is not None else "chat.completion.chunk"
            if choices:
                for choice_data in choices:
                    if isinstance(choice_data, StreamingChoices):
                        actual_choices_list.append(choice_data)
                    elif isinstance(choice_data, dict):
                        actual_choices_list.append(StreamingChoices(**choice_data))
                    # Add handling if choice_data is already a Pydantic model from openai sdk (unlikely here)
            else:
                actual_choices_list.append(StreamingChoices())  # Default if no choices provided
        else:
            # Determine object type for non-streaming responses
            effective_object = object if object is not None else "chat.completion"
            if choices:
                for choice_data in choices:
                    if isinstance(choice_data, Choices):
                        actual_choices_list.append(choice_data)
                    elif isinstance(choice_data, dict):
                        # Ensure 'message' within choice_data is also converted if it's a dict
                        if 'message' in choice_data and isinstance(choice_data['message'], dict):
                            choice_data['message'] = Message(**choice_data['message'])
                        actual_choices_list.append(Choices(**choice_data))
            else:
                actual_choices_list.append(Choices())  # Default if no choices provided

        effective_id = id if id is not None else _generate_id()
        effective_created = created if created is not None else int(time.time())

        effective_usage: Optional[Usage] = None
        if usage is not None:
            if isinstance(usage, dict):
                effective_usage = Usage(**usage)
            elif isinstance(usage, Usage):
                effective_usage = usage
        elif stream is None or stream is False:  # Only create default Usage for non-streaming
            effective_usage = Usage()

        # Values for super().__init__
        init_values = {
            "id": effective_id,
            "choices": actual_choices_list,
            "created": effective_created,
            "model": model,
            "object": effective_object,
            "system_fingerprint": system_fingerprint,
            "stream": stream,
            "stream_options": stream_options,
            "response_ms": response_ms,
            "_hidden_params": hidden_params,  # Note: constructor arg is hidden_params
            "_response_headers": _response_headers,
        }
        if effective_usage is not None:
            init_values["usage"] = effective_usage

        super().__init__(**init_values, **params)


def convert_openai_chat_completion_to_litellm_model_response(
        openai_response: ChatCompletion,
        response_ms: Optional[int] = None,
        hidden_params: Optional[Dict[str, Any]] = None,
        _response_headers: Optional[Dict[str, Any]] = None
) -> ModelResponse:  # Return type is the locally defined ModelResponse
    """
    Converts a non-streaming OpenAI ChatCompletion object to the locally defined ModelResponse object.

    This function processes an openai.types.chat.ChatCompletion object and maps its fields
    to the fields of the ModelResponse class defined within this file.

    Args:
        openai_response: The ChatCompletion object from the OpenAI SDK (non-streaming).
        response_ms: Optional. Response time in milliseconds.
        hidden_params: Optional. LiteLLM-specific hidden parameters.
        _response_headers: Optional. HTTP response headers.

    Returns:
        An instance of the locally defined ModelResponse.
    """

    # Convert OpenAI choices to a list of dictionaries.
    # .model_dump(exclude_none=True) helps by not including keys that are None,
    # allowing Pydantic models to use their defaults if applicable.
    parsed_choices = []
    if openai_response.choices:
        for choice in openai_response.choices:
            # choice.model_dump() will convert the entire choice, including its 'message' attribute (if Pydantic)
            parsed_choices.append(choice.model_dump(exclude_none=True))

    # Convert OpenAI usage object to a dictionary, if it exists.
    parsed_usage = openai_response.usage.model_dump(exclude_none=True) if openai_response.usage else None

    # Instantiate the locally defined ModelResponse.
    # For a "normal" (non-streaming) OpenAI ChatCompletion, 'stream' is False.
    # The 'object' field from the openai_response (e.g., "chat.completion") is passed directly.
    local_model_response = ModelResponse(  # Instantiating the ModelResponse class from this file
        id=openai_response.id,
        choices=parsed_choices,  # This will be processed by ModelResponse.__init__
        created=openai_response.created,
        model=openai_response.model,
        object=openai_response.object,
        system_fingerprint=openai_response.system_fingerprint,
        usage=parsed_usage,  # This will be processed by ModelResponse.__init__
        stream=False,  # Explicitly False for a non-streaming ChatCompletion
        response_ms=response_ms,
        hidden_params=hidden_params,  # Passed to __init__ as hidden_params
        _response_headers=_response_headers
    )

    return local_model_response
</file>

<file path="python-client/main.py">
import os

import litellm
from litellm import CustomLLM, completion, get_llm_provider, acompletion
import asyncio
from dotenv import load_dotenv
load_dotenv()

base_model = os.getenv("BASE_MODEL")
ollama_model = "ollama/" + base_model
litellm_proxy_model = "litellm_proxy/ollama-qwen-local"
litellm_proxy_key = "sk-1234"


print("="*40)
# test we can use the sdk to communicate with ollama directly
resp = completion(
        model=ollama_model,
        messages=[{"role": "user", "content": "Hello world!"}],
        max_tokens=5,
        base_url="http://localhost:11434",
    )

print( resp.choices[0].message.content )
print("="*40)

# test we can communicate with ollama via the litellm proxy
resp = completion(
        model=litellm_proxy_model,
        messages=[{"role": "user", "content": "Hello world!"}],
        max_tokens=5,
        base_url="http://localhost:4000",
        api_key=litellm_proxy_key
    )

print( resp.choices[0].message.content )
print("="*40)

### tests using pass-through custom handler

class OllamaProxyLLM(CustomLLM):
    def completion(self, *args, **kwargs) -> litellm.ModelResponse:
        return litellm.completion(
            model=ollama_model,
            messages=[{"role": "user", "content": "say 'i am synchronous'"}],
            max_tokens=50
        )  # type: ignore

    async def acompletion(self, *args, **kwargs) -> litellm.ModelResponse:
        return litellm.completion(
            model=ollama_model,
            messages=[{"role": "user", "content": "say 'i am asynchronous'"}],
            max_tokens=50
        )  # type: ignore


my_ollama_proxy_llm = OllamaProxyLLM()

litellm.custom_provider_map = [  # 👈 KEY STEP - REGISTER HANDLER
    {"provider": "ollama_proxy_llm", "custom_handler": my_ollama_proxy_llm}
]


# test the proxy works with synchronous calls
resp = completion(
    model="ollama_proxy_llm/my-fake-model",
    messages=[{"role": "user", "content": "Hello world!"}],
)

print(resp.choices[0].message.content)
print("="*40)

# test the proxy works with asynchronous calls
resp = asyncio.run(acompletion(
    model="ollama_proxy_llm/anything-goes-here",
    messages=[{"role": "user", "content": "Hello world!"}],
))

print(resp.choices[0].message.content)
print("="*40)
</file>

<file path="python-client/mtls.py">
import asyncio
from pathlib import Path

import httpx
import os

import litellm
from litellm import api_key, CustomLLM, completion, acompletion
from litellm.types.utils import ModelResponse
from openai.types.chat import ChatCompletion
# from OpenSSL import SSL, X509

current_file_dir = Path(__file__).parent
certs_dir = current_file_dir.parent / "certs"
CERTIFICATE_PATH = certs_dir / "client.crt"
KEY_PATH = certs_dir / "client.key"
CA_PATH = certs_dir / "ca.crt"

for path in (CERTIFICATE_PATH, KEY_PATH, CA_PATH):
    if not path.exists():
        exit("Could not find certificate file {}".format(path))

from dotenv import load_dotenv

load_dotenv()

base_model = os.getenv("BASE_MODEL")
ollama_model = "ollama/" + base_model
LITELLM_PROXY_MODEL = "litellm_proxy/ollama-qwen-local"
LITELLM_PROXY_KEY = "sk-1234"

import ssl
import httpx
import openai


def get_openai_mtls_client(asynchronous=False):
    ctx = ssl.create_default_context(cafile=CA_PATH)
    ctx.load_cert_chain(certfile=CERTIFICATE_PATH, keyfile=KEY_PATH)

    if asynchronous:
        httpx_client = httpx.AsyncClient(verify=ctx)
        openai_client = openai.AsyncOpenAI(http_client=httpx_client,
                                           base_url="https://localhost:8443",
                                           api_key=LITELLM_PROXY_KEY)
    else:
        httpx_client = httpx.Client(verify=ctx)
        openai_client = openai.OpenAI(http_client=httpx_client,
                                      base_url="https://localhost:8443",
                                      api_key=LITELLM_PROXY_KEY)
    return openai_client


# client = get_openai_mtls_client()
#
# response = client.chat.completions.create(
#     model='ollama-qwen-local',
#     messages=[
#         {
#             "role": "user",
#             "content": "Write a one-sentence bedtime story about a unicorn."
#         },
#     ],
#     max_tokens=5
# )
#
# print(response.choices[0].message.content)
# print("=" * 40)

from litellm_utils import convert_openai_chat_completion_to_litellm_model_response


class MTLSOpenAILLM(CustomLLM):
    def completion(self, *args, **kwargs) -> litellm.ModelResponse:
        client = get_openai_mtls_client()
        return convert_openai_chat_completion_to_litellm_model_response(client.chat.completions.create(
            model='ollama-qwen-local',
            messages=[
                {
                    "role": "user",
                    "content": "don't think say 'litellm openai'."
                },
            ],
            max_tokens=15
        ))

    async def acompletion(self, *args, **kwargs) -> litellm.ModelResponse:
        client = get_openai_mtls_client(asynchronous=True)
        return convert_openai_chat_completion_to_litellm_model_response(client.chat.completions.create(
            model='ollama-qwen-local',
            messages=[
                {
                    "role": "user",
                    "content": "don't think say 'litellm openai'."
                },
            ],
            max_tokens=5
        ))


my_mtls_openai_llm = MTLSOpenAILLM()

litellm.custom_provider_map = [  # 👈 KEY STEP - REGISTER HANDLER
    {"provider": "mtls_openai_llm", "custom_handler": my_mtls_openai_llm}
]

# test the proxy works with synchronous calls
resp = completion(
    model="mtls_openai_llm/anything-you-like-here",
    messages=[{"role": "user", "content": "Hello world!"}],
)

print(resp.choices[0].message.content)
print("=" * 40)

resp = asyncio.run(
    acompletion(
    model="mtls_openai_llm/anything-you-like-here",
                               messages=[{"role": "user", "content": "Hello world!"}], ))
print("=" * 40)
</file>

<file path="python-client/sample.py">
import asyncio
from pathlib import Path

import httpx
import os

import litellm
from litellm import api_key, CustomLLM, completion, acompletion
from litellm.types.utils import ModelResponse
from openai.types.chat import ChatCompletion
# from OpenSSL import SSL, X509

current_file_dir = Path(__file__).parent
certs_dir = current_file_dir.parent / "certs"
CERTIFICATE_PATH = certs_dir / "client.crt"
KEY_PATH = certs_dir / "client.key"
CA_PATH = certs_dir / "ca.crt"

for path in (CERTIFICATE_PATH, KEY_PATH, CA_PATH):
    if not path.exists():
        exit("Could not find certificate file {}".format(path))

from dotenv import load_dotenv

load_dotenv()

base_model = os.getenv("BASE_MODEL")
ollama_model = "ollama/" + base_model
LITELLM_PROXY_MODEL = "litellm_proxy/ollama-qwen-local"
LITELLM_PROXY_KEY = "sk-1234"

import ssl
import httpx
import openai


class MTLSOpenAILLM(CustomLLM):
    def completion(self, *args, **kwargs) -> litellm.ModelResponse:
        pass

    async def acompletion(self, *args, **kwargs) -> litellm.ModelResponse:
        pass
    async def acompletion(self, *args, **kwargs) -> litellm.ModelResponse:
        pass

    def streaming(self, *args, **kwargs) -> Iterator[GenericStreamingChunk]:
        """
        calls completion from upstream llm but returns the response as if it was a stream
        """
        pass



    async def astreaming(self, *args, **kwargs) -> AsyncIterator[GenericStreamingChunk]:
        """
        calls acompletion from upstream llm but returns the response as if it was a stream
        """

        pass


my_mtls_openai_llm = MTLSOpenAILLM()

litellm.custom_provider_map = [  # 👈 KEY STEP - REGISTER HANDLER
    {"provider": "mtls_openai_llm", "custom_handler": my_mtls_openai_llm}
]

if __name__ == "__main__":

# test the proxy works with synchronous calls
    resp = completion(
        model="mtls_openai_llm/anything-you-like-here",
        messages=[{"role": "user", "content": "Hello world!"}],
    )

    print(resp.choices[0].message.content)
    print("=" * 40)

    resp = asyncio.run(
        acompletion(
        model="mtls_openai_llm/anything-you-like-here",
                                   messages=[{"role": "user", "content": "Hello world!"}], ))

    print(resp.choices[0].message.content)
    print("=" * 40)

    """
    Streaming demo
    """
    resp = completion(
            model="mtls_openai_llm/anything-you-like-here",
            messages=[{"role": "user", "content": "Hello world!"}],
            streaming=True,
            )
    print(resp.choices[0].message.content)
    print("=" * 40)

    resp = asyncio.run(
        acompletion(
            model="mtls_openai_llm/anything-you-like-here",
            messages=[{"role": "user", "content": "Hello world!"}],
            streaming=True,
            )
    )
    print(resp.choices[0].message.content)
    print("=" * 40)
</file>

<file path="tests/test_litellm_sdk_integration.py">
# test_litellm_integrations.py
import pytest
import litellm
from litellm import CustomLLM, completion, ModelResponse, acompletion
import warnings
import os
from dotenv import load_dotenv
import asyncio
import litellm.exceptions

# Load environment variables from .env file
load_dotenv()


# Fixture to register the custom LLM provider that returns a mock response
@pytest.fixture(scope="module")
def register_custom_llm():
    """
    Registers a custom LLM provider that returns a mock response.
    """

    class MyCustomLLM(CustomLLM):
        # Simplified constructor for ModelResponse, assuming other fields are optional or defaulted by LiteLLM
        def completion(self, *args, **kwargs) -> ModelResponse:
            # Directly create ModelResponse if all required fields for your test are in choices.message.content
            # For a more complete ModelResponse, you'd include id, created, model, etc.
            mock_choice = {"message": litellm.Message(content="Hi!", role="assistant")}
            return ModelResponse(choices=[mock_choice])  # type: ignore

        async def acompletion(self, *args, **kwargs) -> ModelResponse:
            mock_choice = {"message": litellm.Message(content="Hi from async!", role="assistant")}
            return ModelResponse(choices=[mock_choice])  # type: ignore

        def embedding(self, *args, **kwargs):
            raise NotImplementedError("Embedding not implemented for this custom LLM")

    my_custom_llm_instance = MyCustomLLM()
    original_custom_provider_map = litellm.custom_provider_map
    # Ensure current_map is initialized correctly based on original_custom_provider_map's type
    if isinstance(litellm.custom_provider_map, list):
        current_map = list(litellm.custom_provider_map)
    elif isinstance(litellm.custom_provider_map, dict):  # LiteLLM might use a dict
        current_map = litellm.custom_provider_map.copy()
    else:  # Default to a list if it's None or some other type
        current_map = []

    if isinstance(current_map, list):
        current_map.append({"provider": "my-custom-llm", "custom_handler": my_custom_llm_instance})
    elif isinstance(current_map, dict):
        current_map["my-custom-llm"] = my_custom_llm_instance  # If it's a dict mapping provider name to handler

    litellm.custom_provider_map = current_map
    yield
    litellm.custom_provider_map = original_custom_provider_map


# --- Simplified and More Robust Custom LLM for Real Ollama Proxying ---
class OllamaProxyLLMForTest(CustomLLM):
    """
    A custom LLM handler that proxies requests to a real Ollama endpoint.
    This version carefully constructs parameters for inner LiteLLM calls.
    """

    def __init__(self, ollama_model_name: str, ollama_base_url: str = "http://localhost:11434"):
        self.ollama_model_name = ollama_model_name
        self.ollama_base_url = ollama_base_url
        super().__init__()  # Call to parent constructor is important

    def _construct_inner_params(self, **kwargs_from_outer_call) -> dict:
        """
        Helper to construct a clean dictionary of parameters for the inner LiteLLM call.
        Only includes parameters relevant to the Ollama provider.
        """
        params = {
            "model": self.ollama_model_name,
            "messages": kwargs_from_outer_call.get("messages"),
            "base_url": self.ollama_base_url,
        }

        # Define a whitelist of optional parameters that are safe to pass to Ollama
        # These are common parameters for completion calls.
        allowed_optional_params = [
            "max_tokens", "temperature", "top_p", "top_k", "stop",
            "stream", "num_predict", "format", "timeout",
            "presence_penalty", "frequency_penalty", "logit_bias", "extra_headers",
            "metadata"  # metadata can be useful
        ]

        for param_name in allowed_optional_params:
            if param_name in kwargs_from_outer_call:
                params[param_name] = kwargs_from_outer_call[param_name]

        # Ensure a default timeout if not provided
        params.setdefault('timeout', 30)

        return params

    def completion(self, *args, **kwargs) -> ModelResponse:
        # kwargs here are from the initial litellm.completion(model="ollama-proxy-for-test/...", ...) call
        params_for_inner_call = self._construct_inner_params(**kwargs)

        print(
            f"OllamaProxyLLMForTest (sync) calling inner litellm.completion with keys: {list(params_for_inner_call.keys())}")

        # For debugging, you can uncomment the line below:
        # os.environ['LITELLM_LOG'] = 'DEBUG'
        try:
            response = litellm.completion(**params_for_inner_call)
        finally:
            # If you changed LITELLM_LOG, reset it if necessary, e.g., os.environ.pop('LITELLM_LOG', None)
            pass
        return response

    async def acompletion(self, *args, **kwargs) -> ModelResponse:
        # kwargs here are from the initial litellm.acompletion(model="ollama-proxy-for-test/...", ...) call
        # These kwargs will include 'acompletion=True' from the router.
        params_for_inner_call = self._construct_inner_params(**kwargs)

        # Crucially, params_for_inner_call (due to _construct_inner_params whitelist)
        # will NOT contain the 'acompletion' key from the outer kwargs.
        # This is essential to prevent the "multiple values for keyword argument 'acompletion'" error
        # when the inner litellm.acompletion calls functools.partial.

        print(
            f"OllamaProxyLLMForTest (async) calling inner litellm.acompletion with keys: {list(params_for_inner_call.keys())}")

        # os.environ['LITELLM_LOG'] = 'DEBUG' # For debugging
        try:
            response = await litellm.acompletion(**params_for_inner_call)
        finally:
            # os.environ.pop('LITELLM_LOG', None) # Reset if changed
            pass
        return response

    def embedding(self, *args, **kwargs):
        raise NotImplementedError("Embedding not implemented for this custom LLM")


@pytest.fixture(scope="module")
def register_ollama_proxy_llm():
    ollama_model_env = os.getenv("OLLAMA_MODEL_NAME")
    if not ollama_model_env:
        pytest.skip("OLLAMA_MODEL_NAME not set in .env file. Skipping tests that require real Ollama proxy.")
        return

    proxy_instance = OllamaProxyLLMForTest(ollama_model_name=ollama_model_env)
    original_custom_provider_map = litellm.custom_provider_map

    # Handle both list and dict types for custom_provider_map
    if isinstance(litellm.custom_provider_map, list):
        current_map = list(litellm.custom_provider_map)
        # Avoid duplicate registration if tests are re-run in same session (though scope="module" should prevent this)
        if not any(entry.get("provider") == "ollama-proxy-for-test" for entry in current_map):
            current_map.append({"provider": "ollama-proxy-for-test", "custom_handler": proxy_instance})
    elif isinstance(litellm.custom_provider_map, dict):
        current_map = litellm.custom_provider_map.copy()
        current_map["ollama-proxy-for-test"] = proxy_instance
    else:  # Default to list if None or other
        current_map = [{"provider": "ollama-proxy-for-test", "custom_handler": proxy_instance}]

    litellm.custom_provider_map = current_map
    yield
    litellm.custom_provider_map = original_custom_provider_map


# --- Test Cases ---

def test_custom_llm_completion(register_custom_llm):
    """Tests the custom LLM provider with a MOCK response."""
    try:
        resp = completion(
            model="my-custom-llm/my-fake-model",
            messages=[{"role": "user", "content": "Hello world!"}],
        )
        assert resp.choices[0].message.content == "Hi!", "Custom LLM (mock) did not return the expected mock response."
    except Exception as e:
        pytest.fail(f"Custom LLM (mock) completion test failed with an exception: {e}")


def test_ollama_direct_completion():
    """Tests direct completion with an Ollama model."""
    ollama_model = os.getenv("OLLAMA_MODEL_NAME")
    if not ollama_model:
        pytest.skip("OLLAMA_MODEL_NAME not set in .env file. Skipping Ollama direct test.")
        return

    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=DeprecationWarning,
                                message="Use 'content=<...>' to upload raw bytes/text content.", module="httpx._models")
        try:
            print(f"\nAttempting to use Ollama model directly: {ollama_model}")
            resp = completion(
                model=ollama_model,
                messages=[{"role": "user", "content": "Hello world from direct test!"}],
                max_tokens=5,
                base_url="http://localhost:11434",
                timeout=30
            )
            assert resp.choices[0].message.content is not None, "Ollama direct response content is None."
            assert isinstance(resp.choices[0].message.content, str), "Ollama direct response content is not a string."
            assert len(resp.choices[0].message.content) > 0, "Ollama direct response content is empty."
            print(f"Ollama Direct Response ({ollama_model}): {resp.choices[0].message.content}")
        except litellm.exceptions.APIConnectionError as e:
            pytest.fail(
                f"Ollama direct completion test failed. Could not connect to Ollama at http://localhost:11434. Error: {e}")
        except litellm.exceptions.BadRequestError as e:
            pytest.fail(
                f"Ollama direct completion test failed. BadRequestError: Model '{ollama_model}' not available? Error: {e}")
        except Exception as e:
            pytest.fail(f"Ollama direct completion test failed with an unexpected exception: {e}")


def test_litellm_proxy_completion():
    """Tests completion with a LiteLLM Proxy model."""
    litellm_proxy_model = "litellm_proxy/ollama-qwen-local"
    litellm_proxy_key = "sk-1234"

    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=DeprecationWarning,
                                message="Use 'content=<...>' to upload raw bytes/text content.", module="httpx._models")
        try:
            resp = completion(
                model=litellm_proxy_model,
                messages=[{"role": "user", "content": "Hello world from LiteLLM Proxy test!"}],
                max_tokens=5,
                base_url="http://localhost:4000",
                api_key=litellm_proxy_key,
                timeout=30
            )
            assert resp.choices[0].message.content is not None, "LiteLLM Proxy response content is None."
            assert isinstance(resp.choices[0].message.content, str), "LiteLLM Proxy response content is not a string."
            assert len(resp.choices[0].message.content) > 0, "LiteLLM Proxy response content is empty."
            print(f"LiteLLM Proxy Response: {resp.choices[0].message.content}")
        except litellm.exceptions.APIConnectionError as e:
            pytest.fail(
                f"LiteLLM Proxy completion test failed. Could not connect to LiteLLM Proxy at http://localhost:4000. Error: {e}")
        except Exception as e:
            pytest.fail(f"LiteLLM Proxy completion test failed with an unexpected exception: {e}")


def test_custom_ollama_proxy_completion(register_ollama_proxy_llm):
    """Tests synchronous completion through the OllamaProxyLLMForTest (real Ollama call)."""
    if not os.getenv("OLLAMA_MODEL_NAME"):
        pytest.skip("Skipping custom Ollama proxy test as OLLAMA_MODEL_NAME is not set.")
        return

    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=DeprecationWarning,
                                message="Use 'content=<...>' to upload raw bytes/text content.", module="httpx._models")
        try:
            print("\nAttempting custom Ollama proxy (sync) completion...")
            resp = completion(
                model="ollama-proxy-for-test/some-sync-model-id",
                # This model name is for routing to the custom handler
                messages=[{"role": "user", "content": "Why is the sky blue? Answer briefly."}],
                max_tokens=15,
                timeout=45  # Explicit timeout for the test call
            )
            assert resp.choices[0].message.content is not None, "Custom Ollama Proxy (sync) response content is None."
            assert isinstance(resp.choices[0].message.content,
                              str), "Custom Ollama Proxy (sync) response content is not a string."
            assert len(resp.choices[0].message.content) > 0, "Custom Ollama Proxy (sync) response content is empty."
            print(f"Custom Ollama Proxy (Sync) Response: {resp.choices[0].message.content}")
        except litellm.exceptions.APIConnectionError as e:
            pytest.fail(f"Custom Ollama Proxy (sync) test failed to connect to Ollama. Error: {e}")
        except litellm.exceptions.BadRequestError as e:
            pytest.fail(
                f"Custom Ollama Proxy (sync) test failed. BadRequestError: Model not available in Ollama? Error: {e}")
        except Exception as e:
            pytest.fail(f"Custom Ollama Proxy (sync) test failed with an unexpected exception: {e}")


@pytest.mark.asyncio
async def test_custom_ollama_proxy_acompletion(register_ollama_proxy_llm):
    """Tests asynchronous completion through the OllamaProxyLLMForTest (real Ollama call)."""
    if not os.getenv("OLLAMA_MODEL_NAME"):
        pytest.skip("Skipping custom Ollama proxy async test as OLLAMA_MODEL_NAME is not set.")
        return

    with warnings.catch_warnings():
        warnings.filterwarnings("ignore", category=DeprecationWarning,
                                message="Use 'content=<...>' to upload raw bytes/text content.", module="httpx._models")
        try:
            print("\nAttempting custom Ollama proxy (async) acompletion...")
            resp = await acompletion(
                model="ollama-proxy-for-test/some-async-model-id",  # For routing to custom handler
                messages=[{"role": "user", "content": "Tell me a fun fact about otters. Be concise."}],
                max_tokens=20,
                timeout=45  # Explicit timeout for the test call
            )
            assert resp.choices[0].message.content is not None, "Custom Ollama Proxy (async) response content is None."
            assert isinstance(resp.choices[0].message.content,
                              str), "Custom Ollama Proxy (async) response content is not a string."
            assert len(resp.choices[0].message.content) > 0, "Custom Ollama Proxy (async) response content is empty."
            print(f"Custom Ollama Proxy (Async) Response: {resp.choices[0].message.content}")
        except litellm.exceptions.APIConnectionError as e:
            pytest.fail(f"Custom Ollama Proxy (async) test failed to connect to Ollama. Error: {e}")
        except litellm.exceptions.BadRequestError as e:
            pytest.fail(
                f"Custom Ollama Proxy (async) test failed. BadRequestError: Model not available in Ollama? Error: {e}")
        except Exception as e:
            pytest.fail(f"Custom Ollama Proxy (async) test failed with an unexpected exception: {e}")

# Instructions for running tests:
# 1. Ensure OLLAMA_MODEL_NAME is in your .env file (e.g., OLLAMA_MODEL_NAME="ollama/qwen3:0.6b").
# 2. The specified model must be pulled in your Ollama instance.
# 3. Install dependencies: pip install pytest litellm python-dotenv pytest-asyncio httpx
# 4. Ensure Ollama server (http://localhost:11434) and LiteLLM Proxy (http://localhost:4000 for its test) are running.
# 5. Run: pytest -s -v tests/test_litellm_sdk_integration.py
</file>

<file path="tests/test_ollama_setup.py">
import pytest
import requests
import subprocess
import os
import time
from pathlib import Path
import json # For parsing curl output
from dotenv import load_dotenv  # For loading .env file
import ssl # Added for mTLS with OpenAI SDK
import httpx # Added for mTLS with OpenAI SDK
import openai # Added for OpenAI SDK test

# Load environment variables from .env file
load_dotenv()

# --- Configuration ---
BASE_DIR = Path(__file__).resolve().parent.parent
CERTS_DIR = BASE_DIR / "certs"
CLIENT_CERT_PATH = CERTS_DIR / "client.crt"
CLIENT_KEY_PATH = CERTS_DIR / "client.key"
CA_CERT_PATH = CERTS_DIR / "ca.crt"

OLLAMA_DIRECT_URL = "http://127.0.0.1:11434"
OLLAMA_TAGS_ENDPOINT = f"{OLLAMA_DIRECT_URL}/api/tags"
LITELLM_DIRECT_URL = "http://localhost:4000"
LITELLM_DIRECT_CHAT_ENDPOINT = f"{LITELLM_DIRECT_URL}/chat/completions"
MTLS_PROXY_URL = "https://localhost:8443" # Used by OpenAI SDK test
LITELLM_CHAT_ENDPOINT = f"{MTLS_PROXY_URL}/chat/completions" # Used by curl and requests tests

TEST_MODEL_NAME = "ollama-qwen-local" # Used by LiteLLM Direct and OpenAI SDK tests
LITELLM_PROXY_MODEL_FOR_OPENAI_SDK = "ollama-qwen-local" # Explicit model for OpenAI SDK test, matching LiteLLM config
LITELLM_MASTER_KEY = "sk-1234" # Used by all proxy tests

MAX_READINESS_WAIT_SECONDS = 60
READINESS_CHECK_INTERVAL_SECONDS = 5



# --- Helper Functions ---

def run_command(command_list, cwd=None, check=True, capture_output=True, text=True, env_vars=None):
    """Runs a generic command using subprocess."""
    print(f"Running command: {' '.join(command_list)}")
    effective_env = {**os.environ, **(env_vars or {})}
    try:
        process = subprocess.run(
            command_list,
            cwd=cwd or BASE_DIR,
            capture_output=capture_output,
            text=text,
            check=check,
            env=effective_env
        )
        if capture_output:
            if process.stdout and process.stdout.strip():
                print(f"Command stdout:\n{process.stdout.strip()}")
            if process.stderr and process.stderr.strip():
                print(f"Command stderr:\n{process.stderr.strip()}")
        return process
    except subprocess.CalledProcessError as e:
        print(f"Error running command: {' '.join(command_list)}")
        print(f"Return code: {e.returncode}")
        if capture_output:
            if e.stdout and e.stdout.strip():
                print(f"Stdout:\n{e.stdout.strip()}")
            if e.stderr and e.stderr.strip():
                print(f"Stderr:\n{e.stderr.strip()}")
        if check:
            pytest.fail(f"Command failed: {' '.join(command_list)}. Error: {e.stderr or e.stdout or 'Unknown error'}")
    except FileNotFoundError:
        pytest.fail(f"Command {command_list[0]} not found. Is it installed and in PATH?")


def run_docker_compose_command(command_args, env_vars=None, check=True):
    """Wrapper for docker-compose commands."""
    return run_command(["docker-compose"] + command_args, env_vars=env_vars, check=check)


# --- Pytest Fixtures ---

@pytest.fixture(scope="session", autouse=True)
def ensure_services_are_up():
    print("Checking if essential Docker services are running and Ollama is responsive...")
    print(f"Will wait up to {MAX_READINESS_WAIT_SECONDS} seconds for Ollama at {OLLAMA_TAGS_ENDPOINT}.")
    start_time = time.time()
    ollama_ready = False
    last_exception = None
    while time.time() - start_time < MAX_READINESS_WAIT_SECONDS:
        try:
            print(f"Attempting to connect to Ollama at {OLLAMA_TAGS_ENDPOINT} (Attempt {int((time.time() - start_time) / READINESS_CHECK_INTERVAL_SECONDS) + 1})")
            response = requests.get(OLLAMA_TAGS_ENDPOINT, timeout=5)
            response.raise_for_status()
            if response.status_code == 200 and "models" in response.json():
                print("Ollama is responsive and returned model list.")
                ollama_ready = True
                break
            else:
                last_exception = Exception(f"Ollama format unexpected: {response.text[:100]}")
        except requests.exceptions.RequestException as e:
            last_exception = e
            print(f"Error connecting to Ollama or unexpected response. Retrying... Error: {e}")
        time.sleep(READINESS_CHECK_INTERVAL_SECONDS)
    if not ollama_ready:
        pytest.fail(
            f"Ollama service at {OLLAMA_TAGS_ENDPOINT} did not become responsive "
            f"within {MAX_READINESS_WAIT_SECONDS} seconds. Last error: {last_exception}\n"
            "Ensure services are up: 'docker-compose up -d ollama litellm-proxy nginx-mtls-proxy'. "
            "Check logs: 'docker-compose logs ollama'."
        )
    print("Ollama confirmed ready. Giving a brief moment for other services...")
    time.sleep(5)

@pytest.fixture(scope="module")
def regenerate_certificates_and_restart_nginx():
    print("Regenerating certificates with SAN for server and restarting Nginx...")
    CERTS_DIR.mkdir(parents=True, exist_ok=True)
    # Rebuild cert-generator if Dockerfile changed (e.g., new openssl_server.cnf)
    run_docker_compose_command(["build", "cert-generator"])
    run_docker_compose_command(["run", "--rm", "-e", "FORCE_REGENERATE=true", "cert-generator"])
    print("Certificates regenerated.")
    expected_certs = [CA_CERT_PATH, CLIENT_CERT_PATH, CLIENT_KEY_PATH, CERTS_DIR / "server.crt", CERTS_DIR / "server.key"]
    if any(not cert.exists() for cert in expected_certs):
        pytest.fail(f"Missing cert files after generation: {[str(c) for c in expected_certs if not c.exists()]}")
    print("Restarting Nginx proxy...")
    run_docker_compose_command(["restart", "nginx-mtls-proxy"])
    print("Nginx restarted. Allowing a moment for it to initialize...")
    time.sleep(5) # Allow Nginx to fully restart and load configs/certs
    return True

# --- Test Cases ---

def test_ollama_direct_accessible():
    """Test 1: Check if Ollama service is directly accessible."""
    print(f"Testing direct Ollama access at {OLLAMA_TAGS_ENDPOINT}")
    try:
        response = requests.get(OLLAMA_TAGS_ENDPOINT, timeout=10)
        response.raise_for_status()
        assert "models" in response.json(), "Ollama /api/tags response invalid"
        print("Ollama direct access successful.")
    except requests.exceptions.RequestException as e:
        pytest.fail(f"Failed to connect to Ollama directly at {OLLAMA_TAGS_ENDPOINT}: {e}")

@pytest.mark.usefixtures("regenerate_certificates_and_restart_nginx")
def test_certificates_are_present_after_regeneration():
    """Test 2: Verify certificate files are present after regeneration."""
    print("Verifying presence of generated certificate files...")
    assert CA_CERT_PATH.exists(), f"CA certificate missing: {CA_CERT_PATH}"
    assert CLIENT_CERT_PATH.exists(), f"Client certificate missing: {CLIENT_CERT_PATH}"
    assert (CERTS_DIR / "server.crt").exists(), f"Server certificate missing"
    print("All expected certificate files are present.")


class TestLiteLLMDirect:
    def test_litellm_direct_correct_auth(self):
        """Test accessing LiteLLM proxy directly with correct authentication."""
        print(f"Testing LiteLLM direct access with correct auth: {LITELLM_DIRECT_CHAT_ENDPOINT}")
        payload = {
            "model": TEST_MODEL_NAME,
            "messages": [{"role": "user", "content": "What is the capital of Germany?"}],
            "max_tokens": 10
        }
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {LITELLM_MASTER_KEY}"
        }
        try:
            response = requests.post(
                LITELLM_DIRECT_CHAT_ENDPOINT,
                json=payload,
                headers=headers,
                timeout=30
            )
            response.raise_for_status()
            assert "choices" in response.json(), "Response missing 'choices'"
            print("LiteLLM direct access with correct auth successful.")
        except requests.exceptions.RequestException as e:
            pytest.fail(f"Request to LiteLLM proxy failed: {e}")

    def test_litellm_direct_no_auth(self):
        """Test accessing LiteLLM proxy directly without authentication."""
        print(f"Testing LiteLLM direct access with NO auth: {LITELLM_DIRECT_CHAT_ENDPOINT}")
        payload = {
            "model": TEST_MODEL_NAME,
            "messages": [{"role": "user", "content": "Test no auth."}],
            "max_tokens": 5
        }
        headers = {"Content-Type": "application/json"}
        try:
            response = requests.post(
                LITELLM_DIRECT_CHAT_ENDPOINT,
                json=payload,
                headers=headers,
                timeout=10
            )
            assert response.status_code == 401, f"Expected HTTP 401 Unauthorized, got {response.status_code}. Response: {response.text}"
            print("LiteLLM direct access without auth correctly failed with HTTP 401.")
        except requests.exceptions.RequestException as e:
            pytest.fail(f"Unexpected exception with no auth: {e}")

    def test_litellm_direct_incorrect_auth(self):
        """Test accessing LiteLLM proxy directly with incorrect authentication."""
        print(f"Testing LiteLLM direct access with INCORRECT auth: {LITELLM_DIRECT_CHAT_ENDPOINT}")
        payload = {
            "model": TEST_MODEL_NAME,
            "messages": [{"role": "user", "content": "Test incorrect auth."}],
            "max_tokens": 5
        }
        headers = {
            "Content-Type": "application/json",
            "Authorization": "Bearer wrong-key"
        }
        try:
            response = requests.post(
                LITELLM_DIRECT_CHAT_ENDPOINT,
                json=payload,
                headers=headers,
                timeout=10
            )
            assert response.status_code == 401, f"Expected HTTP 401 Unauthorized, got {response.status_code}. Response: {response.text}"
            print("LiteLLM direct access with incorrect auth correctly failed with HTTP 401.")
        except requests.exceptions.RequestException as e:
            pytest.fail(f"Unexpected exception with incorrect auth: {e}")

# --- Requests-based mTLS Tests ---
@pytest.mark.usefixtures("regenerate_certificates_and_restart_nginx")
class TestMTLSWithRequests:
    def test_ollama_via_mtls_proxy_correct_cert_requests(self):
        """Test 3a: (Requests) Access Ollama via mTLS proxy with correct client certificate."""
        print(f"Testing (Requests) Ollama via mTLS proxy with correct cert: {LITELLM_CHAT_ENDPOINT}")
        payload = {"model": TEST_MODEL_NAME, "messages": [{"role": "user", "content": "Capital of France?"}], "max_tokens": 5}
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {LITELLM_MASTER_KEY}"}
        try:
            response = requests.post(
                LITELLM_CHAT_ENDPOINT, json=payload, headers=headers,
                cert=(str(CLIENT_CERT_PATH), str(CLIENT_KEY_PATH)),
                verify=str(CA_CERT_PATH), timeout=30
            )
            response.raise_for_status()
            assert "choices" in response.json(), "Response missing 'choices'"
            print("(Requests) mTLS access with correct cert successful.")
        except requests.exceptions.SSLError as e:
            pytest.fail(f"(Requests) SSL error with correct certs: {e}. Nginx logs: 'docker-compose logs nginx-mtls-proxy'")
        except requests.exceptions.RequestException as e:
            pytest.fail(f"(Requests) Request failed with correct certs: {e}")

    def test_ollama_via_mtls_proxy_no_client_cert_requests(self):
        """Test 4a: (Requests) Attempt mTLS proxy WITHOUT client certificate."""
        print(f"Testing (Requests) Ollama via mTLS proxy with NO client cert: {LITELLM_CHAT_ENDPOINT}")
        payload = {"model": TEST_MODEL_NAME, "messages": [{"role": "user", "content": "Test no client cert."}], "max_tokens": 5}
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {LITELLM_MASTER_KEY}"}
        try:
            response = requests.post(LITELLM_CHAT_ENDPOINT, json=payload, headers=headers, verify=str(CA_CERT_PATH), timeout=10)
            # Nginx returns 400 if client cert is required but not sent
            assert response.status_code == 400, f"Expected HTTP 400, got {response.status_code}. Response: {response.text}"
            print("(Requests) mTLS access without client cert correctly failed with HTTP 400.")
        except requests.exceptions.SSLError:
            # This case might also result in an SSLError before HTTP 400 depending on server/client handshake phase
            print("(Requests) mTLS access without client cert correctly failed with SSLError (server likely closed connection).")
            assert True
        except requests.exceptions.RequestException as e:
            pytest.fail(f"(Requests) Unexpected exception with no client cert: {e}")

    def test_ollama_via_mtls_proxy_server_not_trusted_by_client_requests(self):
        """Test 5a: (Requests) Attempt mTLS proxy when client uses default CAs (server should not be trusted)."""
        print(f"Testing (Requests) mTLS proxy with client cert, client NOT trusting server's custom CA: {LITELLM_CHAT_ENDPOINT}")
        payload = {"model": TEST_MODEL_NAME, "messages": [{"role": "user", "content": "Test server not trusted."}], "max_tokens": 5}
        headers = {"Content-Type": "application/json", "Authorization": f"Bearer {LITELLM_MASTER_KEY}"}
        with pytest.raises(requests.exceptions.SSLError) as excinfo:
            requests.post(
                LITELLM_CHAT_ENDPOINT, json=payload, headers=headers,
                cert=(str(CLIENT_CERT_PATH), str(CLIENT_KEY_PATH)),
                verify=True, timeout=10 # verify=True uses system CAs
            )
        print(f"(Requests) mTLS with untrusted server CA correctly failed: {excinfo.value}")
        assert "certificate verify failed" in str(excinfo.value).lower()

# --- Curl-based mTLS Tests ---
@pytest.mark.usefixtures("regenerate_certificates_and_restart_nginx")
class TestMTLSWithCurl:
    def test_ollama_via_mtls_proxy_correct_cert_curl(self):
        """Test 3b: (curl) Access Ollama via mTLS proxy with correct client certificate."""
        print(f"Testing (curl) Ollama via mTLS proxy with correct cert: {LITELLM_CHAT_ENDPOINT}")
        payload_dict = {"model": TEST_MODEL_NAME, "messages": [{"role": "user", "content": "Capital of France using curl?"}], "max_tokens": 5}
        payload = json.dumps(payload_dict)
        curl_command = [
            "curl", "-s", "-X", "POST", LITELLM_CHAT_ENDPOINT,
            "--cert", str(CLIENT_CERT_PATH),
            "--key", str(CLIENT_KEY_PATH),
            "--cacert", str(CA_CERT_PATH),
            "-H", "Content-Type: application/json",
            "-H", f"Authorization: Bearer {LITELLM_MASTER_KEY}",
            "-d", payload
        ]
        process = run_command(curl_command, check=False)
        assert process.returncode == 0, f"curl command failed with exit code {process.returncode}. Stderr: {process.stderr}"
        try:
            response_data = json.loads(process.stdout)
            assert "choices" in response_data, f"curl response missing 'choices'. Output: {process.stdout}"
            print("(curl) mTLS access with correct cert successful.")
        except json.JSONDecodeError:
            pytest.fail(f"curl output was not valid JSON: {process.stdout}")

    def test_ollama_via_mtls_proxy_no_client_cert_curl(self):
        """Test 4b: (curl) Attempt mTLS proxy WITHOUT client certificate."""
        print(f"Testing (curl) Ollama via mTLS proxy with NO client cert: {LITELLM_CHAT_ENDPOINT}")
        payload_dict = {"model": TEST_MODEL_NAME, "messages": [{"role": "user", "content": "Test no client cert curl."}], "max_tokens": 5}
        payload = json.dumps(payload_dict)
        curl_command = [
            "curl", "-s", "-X", "POST", LITELLM_CHAT_ENDPOINT,
            # No --cert or --key
            "--cacert", str(CA_CERT_PATH),
            "-H", "Content-Type: application/json",
            "-H", f"Authorization: Bearer {LITELLM_MASTER_KEY}",
            "-d", payload,
            "--fail-with-body" # Makes curl exit with 22 on 4xx/5xx errors
        ]
        process = run_command(curl_command, check=False)

        # Nginx returns 400 if client cert is required but not sent. curl with --fail-with-body exits with 22 for 400.
        # Other SSL errors might result in different codes (e.g., 35 for handshake failure).
        if process.returncode != 0:
            print(f"(curl) mTLS access without client cert correctly failed with exit code {process.returncode}. Stderr: {process.stderr}. Stdout: {process.stdout}")
            assert process.returncode in [22, 35, 56], f"Expected curl to fail (e.g. exit 22 for HTTP 400, 35 for SSL handshake, 56 for recv error), got {process.returncode}"
            if process.returncode == 22: # HTTP 400
                 assert "400 Bad Request" in process.stdout or "No required SSL certificate was sent" in process.stdout
        else:
            pytest.fail(f"(curl) mTLS access without client cert unexpectedly succeeded (exit 0). Output: {process.stdout}")


    def test_ollama_via_mtls_proxy_server_not_trusted_by_client_curl(self):
        """Test 5b: (curl) Attempt mTLS proxy when client uses default CAs (server should not be trusted)."""
        print(f"Testing (curl) mTLS proxy with client cert, client NOT trusting server's custom CA: {LITELLM_CHAT_ENDPOINT}")
        payload_dict = {"model": TEST_MODEL_NAME, "messages": [{"role": "user", "content": "Test server not trusted curl."}], "max_tokens": 5}
        payload = json.dumps(payload_dict)
        curl_command = [
            "curl", "-s", "-X", "POST", LITELLM_CHAT_ENDPOINT,
            "--cert", str(CLIENT_CERT_PATH),
            "--key", str(CLIENT_KEY_PATH),
            # NO --cacert str(CA_CERT_PATH) means curl uses its default CA bundle
            "-H", "Content-Type: application/json",
            "-H", f"Authorization: Bearer {LITELLM_MASTER_KEY}",
            "-d", payload
        ]
        process = run_command(curl_command, check=False)
        # Curl exit code 60: Peer certificate cannot be authenticated with known CA certificates.
        assert process.returncode == 60, \
            f"(curl) Expected SSL verification failure (exit code 60), got {process.returncode}. Stderr: {process.stderr}, Stdout: {process.stdout}"
        print(f"(curl) mTLS with untrusted server CA correctly failed with exit code {process.returncode}.")

# --- OpenAI SDK mTLS Test ---
@pytest.mark.usefixtures("regenerate_certificates_and_restart_nginx")
class TestMTLSWithOpenAISDK:
    def test_ollama_via_mtls_proxy_openai_sdk(self):
        """Test accessing Ollama via mTLS proxy using the OpenAI SDK with a custom httpx client."""
        print(f"Testing (OpenAI SDK) Ollama via mTLS proxy: {MTLS_PROXY_URL}")

        # Ensure certificate files exist (fixture should handle this, but good for clarity)
        # CORRECTED: Use the globally defined CLIENT_CERT_PATH, CLIENT_KEY_PATH, CA_CERT_PATH
        for cert_file in (CLIENT_CERT_PATH, CLIENT_KEY_PATH, CA_CERT_PATH):
            if not cert_file.exists():
                pytest.fail(f"Certificate file missing for OpenAI SDK test: {cert_file}")

        try:
            # Create SSL context for mTLS
            # For client-side, Purpose.SERVER_AUTH is used to verify the server's certificate against cafile.
            # The client's own certificate is loaded with load_cert_chain.
            # CORRECTED: Use the globally defined CA_CERT_PATH, CLIENT_CERT_PATH, CLIENT_KEY_PATH
            ctx = ssl.create_default_context(ssl.Purpose.SERVER_AUTH, cafile=str(CA_CERT_PATH))
            ctx.load_cert_chain(certfile=str(CLIENT_CERT_PATH), keyfile=str(CLIENT_KEY_PATH))
            # Optionally, enforce server hostname verification if not default
            # ctx.check_hostname = True
            # ctx.verify_mode = ssl.CERT_REQUIRED


            # Create httpx client with the SSL context
            # The 'verify' parameter in httpx.Client takes the SSLContext for custom verification.
            http_client = httpx.Client(verify=ctx)

            # Initialize OpenAI client
            openai_api_key = LITELLM_MASTER_KEY # Re-use the master key for the proxy
            openai_base_url = MTLS_PROXY_URL # Nginx mTLS proxy URL

            client = openai.OpenAI(
                http_client=http_client,
                base_url=openai_base_url,
                api_key=openai_api_key
            )

            # Make the chat completion request
            print(f"Making request to model: {LITELLM_PROXY_MODEL_FOR_OPENAI_SDK} via OpenAI SDK")
            completion_response = client.chat.completions.create(
                model=LITELLM_PROXY_MODEL_FOR_OPENAI_SDK, # This should match a model_name in litellm_proxy/config.yaml
                messages=[
                    {
                        "role": "user",
                        "content": "Write a one-sentence bedtime story about a brave knight."
                    }
                ],
                max_tokens=5 # Keep the test quick
            )

            # Assertions
            assert completion_response is not None, "OpenAI SDK completion response is None."
            assert len(completion_response.choices) > 0, "OpenAI SDK response has no choices."
            message = completion_response.choices[0].message
            assert message is not None, "OpenAI SDK response message is None."
            assert message.content is not None and len(message.content.strip()) > 0, \
                "OpenAI SDK response message content is empty or None."

            print(f"(OpenAI SDK) mTLS access successful. Response: {message.content[:100]}...")

        except httpx.ConnectError as e:
            # This can happen if Nginx is not up, or SSL handshake fails at a very low level.
            # Nginx logs (docker-compose logs nginx-mtls-proxy) would be helpful.
            pytest.fail(f"(OpenAI SDK) httpx.ConnectError: {e}. Is Nginx proxy running and configured correctly for mTLS on {MTLS_PROXY_URL}?")
        except openai.APIConnectionError as e:
            # More specific OpenAI SDK error for connection issues
            pytest.fail(f"(OpenAI SDK) APIConnectionError: {e}. Check proxy and network.")
        except openai.AuthenticationError as e:
            # Should not happen if LITELLM_MASTER_KEY is correct for the proxy
            pytest.fail(f"(OpenAI SDK) AuthenticationError: {e}. Check API key.")
        except openai.APIStatusError as e:
            # General API error from the server (e.g. 4xx, 5xx from the proxy/LiteLLM)
            pytest.fail(f"(OpenAI SDK) APIStatusError: status={e.status_code}, response={e.response.text[:200] if e.response else 'N/A'}")
        except Exception as e:
            # Catch any other unexpected errors
            pytest.fail(f"(OpenAI SDK) An unexpected error occurred: {type(e).__name__}: {e}")
        finally:
            # Ensure the httpx client is closed if it was created
            if 'http_client' in locals() and http_client: # Check if http_client was defined
                http_client.close()
</file>

<file path=".gitignore">
.idea
certs/*
.idea-old
.venv-old
**/__pycache__/
.env
</file>

<file path="custom_handler_instructions.md">
Custom API Server (Custom Format)Call your custom torch-serve / internal LLM APIs via LiteLLMinfoFor calling an openai-compatible endpoint, go here.For modifying incoming/outgoing calls on proxy, go here.Quick Startimport litellm
from litellm import CustomLLM, completion, get_llm_provider


class MyCustomLLM(CustomLLM):
    def completion(self, *args, **kwargs) -> litellm.ModelResponse:
        return litellm.completion(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Hello world"}],
            mock_response="Hi!",
        )  # type: ignore

my_custom_llm = MyCustomLLM()

litellm.custom_provider_map = [ # 👈 KEY STEP - REGISTER HANDLER
        {"provider": "my-custom-llm", "custom_handler": my_custom_llm}
    ]

resp = completion(
        model="my-custom-llm/my-fake-model",
        messages=[{"role": "user", "content": "Hello world!"}],
    )

assert resp.choices[0].message.content == "Hi!"
OpenAI Proxy UsageSetup your custom_handler.py fileimport litellm
from litellm import CustomLLM, completion, get_llm_provider


class MyCustomLLM(CustomLLM):
    def completion(self, *args, **kwargs) -> litellm.ModelResponse:
        return litellm.completion(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Hello world"}],
            mock_response="Hi!",
        )  # type: ignore

    async def acompletion(self, *args, **kwargs) -> litellm.ModelResponse:
        return litellm.completion(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Hello world"}],
            mock_response="Hi!",
        )  # type: ignore


my_custom_llm = MyCustomLLM()
Add to config.yamlIn the config below, we passpython_filename: custom_handler.pycustom_handler_instance_name: my_custom_llm. This is defined in Step 1custom_handler: custom_handler.my_custom_llmmodel_list:
  - model_name: "test-model"
    litellm_params:
      model: "openai/text-embedding-ada-002"
  - model_name: "my-custom-model"
    litellm_params:
      model: "my-custom-llm/my-model"

litellm_settings:
  custom_provider_map:
  - {"provider": "my-custom-llm", "custom_handler": custom_handler.my_custom_llm}
```bash
litellm --config /path/to/config.yaml
Test it!curl -X POST '[http://0.0.0.0:4000/chat/completions](http://0.0.0.0:4000/chat/completions)' \
-H 'Content-Type: application/json' \
-H 'Authorization: Bearer sk-1234' \
-d '{
    "model": "my-custom-model",
    "messages": [{"role": "user", "content": "Say \"this is a test\" in JSON!"}],
}'
Expected Response{
    "id": "chatcmpl-06f1b9cd-08bc-43f7-9814-a69173921216",
    "choices": [
        {
            "finish_reason": "stop",
            "index": 0,
            "message": {
                "content": "Hi!",
                "role": "assistant",
                "tool_calls": null,
                "function_call": null
            }
        }
    ],
    "created": 1721955063,
    "model": "gpt-3.5-turbo",
    "object": "chat.completion",
    "system_fingerprint": null,
    "usage": {
        "prompt_tokens": 10,
        "completion_tokens": 20,
        "total_tokens": 30
    }
}
Add Streaming SupportHere's a simple example of returning unix epoch seconds for both completion + streaming use-cases.s/o @Eloy Lafuente for this code example.import time
from typing import Iterator, AsyncIterator
from litellm.types.utils import GenericStreamingChunk, ModelResponse
from litellm import CustomLLM, completion, acompletion

class UnixTimeLLM(CustomLLM):
    def completion(self, *args, **kwargs) -> ModelResponse:
        return completion(
            model="test/unixtime",
            mock_response=str(int(time.time())),
        )  # type: ignore

    async def acompletion(self, *args, **kwargs) -> ModelResponse:
        return await acompletion(
            model="test/unixtime",
            mock_response=str(int(time.time())),
        )  # type: ignore

    def streaming(self, *args, **kwargs) -> Iterator[GenericStreamingChunk]:
        generic_streaming_chunk: GenericStreamingChunk = {
            "finish_reason": "stop",
            "index": 0,
            "is_finished": True,
            "text": str(int(time.time())),
            "tool_use": None,
            "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
        }
        yield generic_streaming_chunk # type: ignore # Corrected: should yield, not return

    async def astreaming(self, *args, **kwargs) -> AsyncIterator[GenericStreamingChunk]:
        generic_streaming_chunk: GenericStreamingChunk = {
            "finish_reason": "stop",
            "index": 0,
            "is_finished": True,
            "text": str(int(time.time())),
            "tool_use": None,
            "usage": {"completion_tokens": 0, "prompt_tokens": 0, "total_tokens": 0},
        }
        yield generic_streaming_chunk # type: ignore

unixtime = UnixTimeLLM()
Image GenerationSetup your custom_handler.py fileimport litellm
from litellm import CustomLLM
from litellm.types.utils import ImageResponse, ImageObject
import time # Added import
from typing import Any, Optional, Union # Added import
import httpx # Added import
from litellm.llms.base import AsyncHTTPHandler # Added import


class MyCustomLLM(CustomLLM):
    async def aimage_generation(self, model: str, prompt: str, model_response: ImageResponse, optional_params: dict, logging_obj: Any, timeout: Optional[Union[float, httpx.Timeout]] = None, client: Optional[AsyncHTTPHandler] = None,) -> ImageResponse:
        return ImageResponse(
            created=int(time.time()),
            data=[ImageObject(url="[https://example.com/image.png](https://example.com/image.png)")],
        )

my_custom_llm = MyCustomLLM()
Add to config.yamlIn the config below, we passpython_filename: custom_handler.pycustom_handler_instance_name: my_custom_llm. This is defined in Step 1custom_handler: custom_handler.my_custom_llmmodel_list:
  - model_name: "test-model"
    litellm_params:
      model: "openai/text-embedding-ada-002"
  - model_name: "my-custom-model"
    litellm_params:
      model: "my-custom-llm/my-model"

litellm_settings:
  custom_provider_map:
  - {"provider": "my-custom-llm", "custom_handler": custom_handler.my_custom_llm}
```bash
litellm --config /path/to/config.yaml
Test it!curl -X POST 'http://0.0.0.0:4000/v1/images/generations' \
-H 'Content-Type: application/json' \
-H 'Authorization: Bearer sk-1234' \
-d '{
    "model": "my-custom-model",
    "prompt": "A cute baby sea otter",
}'
Expected Response{
    "created": 1721955063,
    "data": [{"url": "https://example.com/image.png"}],
}
Additional ParametersAdditional parameters are passed inside optional_params key in the completion or image_generation function.Here's how to set this:import litellm
from litellm import CustomLLM, completion, get_llm_provider


class MyCustomLLM(CustomLLM):
    def completion(self, *args, **kwargs) -> litellm.ModelResponse:
        assert kwargs["optional_params"] == {"my_custom_param": "my-custom-param"} # 👈 CHECK HERE
        return litellm.completion(
            model="gpt-3.5-turbo",
            messages=[{"role": "user", "content": "Hello world"}],
            mock_response="Hi!",
        )  # type: ignore

my_custom_llm = MyCustomLLM()

litellm.custom_provider_map = [ # 👈 KEY STEP - REGISTER HANDLER
        {"provider": "my-custom-llm", "custom_handler": my_custom_llm}
    ]

resp = completion(model="my-custom-llm/my-model", my_custom_param="my-custom-param")
Setup your custom_handler.py fileimport litellm
from litellm import CustomLLM
from litellm.types.utils import ImageResponse, ImageObject
import time # Added import
from typing import Any, Optional, Union # Added import
import httpx # Added import
from litellm.llms.base import AsyncHTTPHandler # Added import


class MyCustomLLM(CustomLLM):
    async def aimage_generation(self, model: str, prompt: str, model_response: ImageResponse, optional_params: dict, logging_obj: Any, timeout: Optional[Union[float, httpx.Timeout]] = None, client: Optional[AsyncHTTPHandler] = None,) -> ImageResponse:
        assert optional_params == {"my_custom_param": "my-custom-param"} # 👈 CHECK HERE
        return ImageResponse(
            created=int(time.time()),
            data=[ImageObject(url="https://example.com/image.png")],
        )

my_custom_llm = MyCustomLLM()
Add to config.yamlIn the config below, we passpython_filename: custom_handler.pycustom_handler_instance_name: my_custom_llm. This is defined in Step 1custom_handler: custom_handler.my_custom_llmmodel_list:
  - model_name: "test-model"
    litellm_params:
      model: "openai/text-embedding-ada-002"
  - model_name: "my-custom-model"
    litellm_params:
      model: "my-custom-llm/my-model"
      my_custom_param: "my-custom-param" # 👈 CUSTOM PARAM

litellm_settings:
  custom_provider_map:
  - {"provider": "my-custom-llm", "custom_handler": custom_handler.my_custom_llm}
```bash
litellm --config /path/to/config.yaml
Test it!curl -X POST '[http://0.0.0.0:4000/v1/images/generations](http://0.0.0.0:4000/v1/images/generations)' \
-H 'Content-Type: application/json' \
-H 'Authorization: Bearer sk-1234' \
-d '{
    "model": "my-custom-model",
    "prompt": "A cute baby sea otter",
}'
Custom Handler Specfrom litellm.types.utils import GenericStreamingChunk, ModelResponse, ImageResponse
from typing import Iterator, AsyncIterator, Any, Optional, Union
from litellm.llms.base import BaseLLM, HTTPHandler, AsyncHTTPHandler # Added imports
import httpx # Added import


class CustomLLMError(Exception):  # use this for all your exceptions
    def __init__(
        self,
        status_code,
        message,
    ):
        self.status_code = status_code
        self.message = message
        super().__init__(
            self.message
        )  # Call the base class constructor with the parameters it needs

class CustomLLM(BaseLLM):
    def __init__(self) -> None:
        super().__init__()

    def completion(self, *args, **kwargs) -> ModelResponse:
        raise CustomLLMError(status_code=500, message="Not implemented yet!")

    def streaming(self, *args, **kwargs) -> Iterator[GenericStreamingChunk]:
        raise CustomLLMError(status_code=500, message="Not implemented yet!")

    async def acompletion(self, *args, **kwargs) -> ModelResponse:
        raise CustomLLMError(status_code=500, message="Not implemented yet!")

    async def astreaming(self, *args, **kwargs) -> AsyncIterator[GenericStreamingChunk]:
        raise CustomLLMError(status_code=500, message="Not implemented yet!")

    def image_generation(
        self,
        model: str,
        prompt: str,
        model_response: ImageResponse,
        optional_params: dict,
        logging_obj: Any,
        timeout: Optional[Union[float, httpx.Timeout]] = None,
        client: Optional[HTTPHandler] = None,
    ) -> ImageResponse:
        raise CustomLLMError(status_code=500, message="Not implemented yet!")

    async def aimage_generation(
        self,
        model: str,
        prompt: str,
        model_response: ImageResponse,
        optional_params: dict,
        logging_obj: Any,
        timeout: Optional[Union[float, httpx.Timeout]] = None,
        client: Optional[AsyncHTTPHandler] = None,
    ) -> ImageResponse:
        raise CustomLLMError(status_code=500, message="Not implemented yet!")
</file>

<file path="docker-compose.yml">
# docker-compose.yml
version: '3.8'

services:
  cert-generator:
    build: ./cert-generator
    container_name: local_cert_generator
    volumes:
      # Mounts the local ./certs directory to /certs_output inside the container
      # The generate_certs.sh script will copy the generated certs here.
      - ./certs:/certs_output # Output directory for generated certs
    environment:
     - FORCE_REGENERATE=true # Uncomment to force regeneration

  nginx-mtls-proxy:
    image: nginx:alpine
    container_name: local_mtls_proxy
    ports:
      - "8443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./certs:/etc/nginx/certs:ro # Nginx uses certs from here
    networks:
      - mtls-test-net
    depends_on:
      - litellm-proxy

  litellm-proxy:
    image: ghcr.io/berriai/litellm:main-latest # Consider pinning to a specific stable version later
    container_name: local_litellm_proxy
    expose:
      - "4000"
    ports:
      - "4000:4000"
    volumes:
      - ./litellm_proxy/config.yaml:/app/config.yaml:ro
    command: [ "--config", "/app/config.yaml", "--port", "4000", "--host", "0.0.0.0" ]
    networks:
      - mtls-test-net
    depends_on:
      - ollama # LiteLLM waits for Ollama
    environment: # <-- Add this section
      - LITELLM_MASTER_KEY=sk-1234

  ollama:
    build: ./ollama # Build from the custom Ollama Dockerfile
    container_name: local_ollama
    expose:
      - "11434"
    ports:
      - "11434:11434"
    networks:
      - mtls-test-net
    environment:
      - OLLAMA_PULL_MODEL=qwen3:0.6b # Specify model here, used by entrypoint.sh
      # Set OLLAMA_MODELS to the path where models will be stored inside the container.
      # This should match the volume mount path.
      - OLLAMA_MODELS=/root/.ollama/models
    volumes:
      # Mount the named volume 'ollama_models_cache' to /root/.ollama inside the container.
      # This directory is where Ollama stores its models by default.
      - ollama_models_cache:/root/.ollama
    # deploy: # Optional GPU support
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

networks:
  mtls-test-net:
    driver: bridge

# Define the named volume for caching Ollama models
volumes:
  ollama_models_cache:
    driver: local # Specifies the local driver, which is the default
</file>

<file path="imports.py">
from litellm import completion, acompletion
from litellm.llms.azure.azure import AzureChatCompletion
from litellm.llms.base import BaseLLM
from litellm.llms.custom_httpx.http_handler import AsyncHTTPHandler, HTTPHandler
from litellm.types.utils import GenericStreamingChunk, ImageResponse, ModelResponse
from openai import OpenAI, AzureOpenAI
</file>

<file path="instructions.md">
I have this project in which I'm trying to build a custom handler for LiteLLM that will do two things:



 - provide mtls authentication to my custom openapi endpoint
 - patch the streaming method so that it returns what looks like a stream, but, under the hood, it will acutally just call an async completion from my upstream llm provider and return it as a single chunk (my upstream provider doesn't support streaming but i want that to be transparent to my client app)
</file>

<file path="README.md">
# Local mTLS Testing Environment for LLM Services

This setup uses Docker Compose to create a local environment with:
1. An Nginx reverse proxy enforcing mTLS.
2. An Ollama service running a local LLM (e.g., `qwen2:0.5b` by default).
3. A LiteLLM Proxy service routing requests to Ollama.
4. A utility container to generate self-signed mTLS certificates.

This allows testing of clients (like a custom LiteLLM provider) that need to communicate with a backend service requiring mTLS.

## Prerequisites

- Docker
- Docker Compose
- `curl` (for testing)

## Setup and Usage

### 1. Generate mTLS Certificates (One-time or if certs expire/change)

The `cert-generator` service in `docker-compose.yml` is responsible for this. The generated certificates will be placed in the `./certs` directory on your host machine.

**To generate certificates:**

a. Ensure the `./certs` directory exists in your `local-mtls-test` project root. If not, create it:
   ```bash
   mkdir -p certs
   ```

b. Run the certificate generator service. This command will build the `cert-generator` image if it doesn't exist and then run it. The container will exit after generating the certs.
   ```bash
   docker-compose up cert-generator
   ```

   This will execute the `generate_certs.sh` script inside the container, creating:
   - `ca.crt`, `ca.key` (Certificate Authority)
   - `server.crt`, `server.key` (for the Nginx mTLS proxy)
   - `client.crt`, `client.key` (for your test client/application)

   These files will be copied to your local `./certs` directory.

c. **To force regeneration** (e.g., if you change subject names or want new certs):
   You can either:
   - Delete the contents of your local `./certs` directory and re-run `docker-compose up cert-generator`.
   - Or, uncomment the `FORCE_REGENERATE=true` environment variable in the `cert-generator` service definition within `docker-compose.yml` and then run `docker-compose up cert-generator`. Remember to comment it out again afterward if you don't want to force regeneration every time.

### 2. Start the Main Services

Once certificates are generated and present in the `./certs` directory, start the Nginx, LiteLLM Proxy, and Ollama services:

```bash
docker-compose up -d nginx-mtls-proxy litellm-proxy ollama
```

The ollama service will automatically pull the model specified by OLLAMA_PULL_MODEL in docker-compose.yml (default: qwen2:0.5b) on its first startup after a fresh build or if the model isn't cached. This might take a few minutes.

Check logs to ensure services start correctly:
```bash
docker-compose logs -f
```

Look for Uvicorn running on http://0.0.0.0:4000 from local_litellm_proxy and messages indicating Ollama has pulled the model and is listening from local_ollama. Nginx logs (local_mtls_proxy) should show it's ready.

### 3. Test mTLS Endpoint with curl

The Nginx mTLS proxy listens on https://localhost:8443. The LiteLLM Proxy uses ollama-qwen-local as the model name for the qwen2:0.5b model (or whatever model is configured in litellm_proxy/config.yaml and pulled by Ollama).

**Important**: Run these curl commands from your host machine, in the local-mtls-test directory (so the relative paths to certs/ are correct).

a) Test WITH Valid Client Certificate (Successful Case):
This command tells curl to use the client certificate and key, and to trust your custom CA for validating the server's certificate.

```bash
curl -X POST https://localhost:8443/chat/completions \
  --cert certs/client.crt \
  --key certs/client.key \
  --cacert certs/ca.crt \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-1234" \
  -d '{
    "model": "ollama-qwen-local",
    "messages": [{"role": "user", "content": "Briefly, what is the capital of France?"}]
  }'
```

Expected Output: A JSON response from the Ollama model (e.g., {"... "content": "Paris is the capital of France." ...}).

b) Test WITHOUT Client Certificate (mTLS Failure Case):
Nginx is configured to require a client certificate (ssl_verify_client on), so it should reject the TLS handshake or return an error. Adding -v gives verbose output.

```bash
curl -X POST https://localhost:8443/chat/completions \
  --cacert certs/ca.crt \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-1234" \
  -d '{
    "model": "ollama-qwen-local",
    "messages": [{"role": "user", "content": "Test no client cert."}]
  }' \
  -v
```

**Expected Output:** A TLS handshake failure. Look for messages like:
- `* OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to localhost:8443`
- `* schannel: failed to decrypt data, data not available`
- `* NSS: client certificate not found (nickname not specified)`
- `curl: (35) ... ssl handshake failure`

Or, Nginx might return an HTTP 400 error: `<html><head><title>400 No required SSL certificate was sent</title></head>...</html>`.

c) Test WITH Client Certificate but Server Cert Not Trusted by Client (TLS Failure Case):
This simulates if your client doesn't trust the CA that signed the Nginx server's certificate. We achieve this by *not* providing `--cacert` to `curl`.

```bash
curl -X POST https://localhost:8443/chat/completions \
  --cert certs/client.crt \
  --key certs/client.key \
  # --cacert certs/ca.crt  <-- CA certificate is NOT provided to curl
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer sk-1234" \
  -d '{
    "model": "ollama-qwen-local",
    "messages": [{"role": "user", "content": "Test server trust fail."}]
  }' \
  -v
```

Expected Output: A certificate verification error from curl because it cannot verify the Nginx server's certificate against its known CAs (since our custom CA isn't in the system's default trust store).
* SSL certificate problem: self-signed certificate in certificate chain (or similar, as our CA is self-signed and not known to curl without --cacert)
* curl: (60) SSL certificate problem...

### 4. Configure Your Main Python Application (for mTLS Provider Testing)

To test your application's AzureAPIMMTLSProvider against this local setup:

Environment Variables for your Python App:
Set these where your Python application runs (e.g., in your .env file for the main project, adjusting the paths as necessary if your main app is not in the same parent directory as local-mtls-test):

```
# Example if local-mtls-test is a sibling to your app's root
# AZURE_APIM_MTLS_CERT_PATH=../local-mtls-test/certs/client.crt
# AZURE_APIM_MTLS_KEY_PATH=../local-mtls-test/certs/client.key

# Or, if your main app is at /path/to/main-app and local-mtls-test is at /path/to/local-mtls-test
# AZURE_APIM_MTLS_CERT_PATH=/path/to/local-mtls-test/certs/client.crt
# AZURE_APIM_MTLS_KEY_PATH=/path/to/local-mtls-test/certs/client.key

# For simplicity during initial testing, you can use absolute paths:
# AZURE_APIM_MTLS_CERT_PATH=/full/path/to/your/local-mtls-test/certs/client.crt
# AZURE_APIM_MTLS_KEY_PATH=/full/path/to/your/local-mtls-test/certs/client.key
```

**Important**: Ensure these paths are resolvable from the directory where your Python application process is running.

llm_providers.json in your Python App:
Configure the provider to point to the local Nginx mTLS proxy. The provider_type must match CUSTOM_PROVIDER_NAME from custom_llm_providers.py.

```json
{
  // ... other providers ...
  "test-local-mtls-ollama": {
    "display_name": "Local mTLS Ollama (Qwen)",
    "provider_type": "azure-apim-mtls", // Your custom provider's registered name
    "deployment_name": "ollama-qwen-local", // Must match model_name in litellm_proxy/config.yaml
    "api_base": "https://localhost:8443",    // Nginx mTLS endpoint
    "api_key": "sk-1234", // The master_key for LiteLLM Proxy auth (passed to custom provider)
    "api_version": "2024-02-01" // Dummy version if your custom provider expects/needs it
  }
  // ...
}
```

Run your Python application. When you make a request using the test-local-mtls-ollama model key, your AzureAPIMMTLSProvider should handle the mTLS connection to https://localhost:8443.

### 5. Stopping the Services

When you're done testing:
```bash
docker-compose down
```

This stops and removes the containers. The `./certs` directory (and its contents) will persist on your host machine.

To also remove the Docker network if it's not needed by other Compose projects:
```bash
docker-compose down --remove-orphans
```

## Additional Notes

This environment provides a way to locally test mTLS interactions. The AzureAPIMMTLSProvider's SSL context is configured to load default CAs for server certificate verification, which works for publicly trusted CAs. For this local setup with a self-signed CA, your client application (Python with httpx) would typically need to be configured to trust certs/ca.crt if it were verifying the server cert itself directly. However, the httpx.Client(verify=ssl_context) where ssl_context only has load_cert_chain (client cert) and load_default_certs relies on those default CAs. If localhost's cert isn't signed by one of those, you might need to adjust the client's ssl_context to also trust your custom ca.crt for the server verification part, or use verify=path/to/your/ca.crt in httpx.Client. The curl example explicitly uses --cacert for this reason.

For the AzureAPIMMTLSProvider, the ssl.create_default_context(ssl.Purpose.SERVER_AUTH) combined with ssl_context.load_default_certs() should be sufficient if the server (Nginx) presents a cert verifiable by standard CAs. For our local Nginx using a custom CA, the httpx.Client(verify=ssl_context) in the custom provider will need that ssl_context to also trust our ca.crt. Let's ensure the custom provider's _create_ssl_context does this.

Refinement for AzureAPIMMTLSProvider._create_ssl_context (in your main app's custom_llm_providers.py) to trust the local CA:
If you encounter SSL verification errors when your Python app connects to https://localhost:8443 (because localhost's cert is signed by your custom CA which isn't in the default trust store), you'll need to tell httpx to trust your CA for server certificate validation.

In app/core/custom_llm_providers.py (your main application, not the test setup):

```python
# In AzureAPIMMTLSProvider, method _create_ssl_context:
# ...
    def _create_ssl_context(self) -> ssl.SSLContext:
        try:
            # For SERVER_AUTH, we need to specify the CA that signed the SERVER's cert
            # if it's not a publicly trusted one.
            # For this local test, our server (Nginx) uses a cert signed by our ca.crt
            # So, we need to load ca.crt for server verification.
            ca_for_server_verification_path = os.path.join(
                os.path.dirname(self.cert_path), # Assuming ca.crt is in the same dir as client.crt
                "ca.crt" # Or get this path from another env var / config
            )
            if not os.path.exists(ca_for_server_verification_path):
                logger.warning(f"CA certificate for server verification not found at {ca_for_server_verification_path}. SSL/TLS server verification might fail if server uses a custom CA.")
                # Fallback to default CAs only if our specific one isn't found
                ssl_context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)
                ssl_context.load_default_certs()
            else:
                logger.info(f"Loading custom CA for server verification: {ca_for_server_verification_path}")
                ssl_context = ssl.create_default_context(
                    ssl.Purpose.SERVER_AUTH,
                    cafile=ca_for_server_verification_path
                )

            ssl_context.load_cert_chain(certfile=self.cert_path, keyfile=self.key_path) # Client cert
            ssl_context.check_hostname = True
            ssl_context.verify_mode = ssl.CERT_REQUIRED # Verify server's cert
            return ssl_context
        # ... (rest of the error handling)
# ...
```

This refinement to _create_ssl_context in your actual application's custom provider will be important when testing against the local Nginx server that uses a certificate signed by your custom ca.crt. You'd also need to ensure ca.crt is available to your main application (e.g., copy it from local-mtls-test/certs/ to a location your app can access).
</file>

<file path="repomix.sh">
repomix -o project_contents.txt -i .jj,.venv
</file>

<file path="requirements.txt">
pytest
requests
litellm
openai
python-dotenv
pytest-asyncio
httpx
httpx[http2]
</file>

</files>
