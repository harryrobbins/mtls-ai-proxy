# docker-compose.yml
version: '3.8'

services:
  cert-generator:
    build: ./cert-generator
    container_name: local_cert_generator
    volumes:
      # Mounts the local ./certs directory to /certs_output inside the container
      # The generate_certs.sh script will copy the generated certs here.
      - ./certs:/certs_output # Output directory for generated certs
    environment:
     - FORCE_REGENERATE=true # Uncomment to force regeneration

  nginx-mtls-proxy:
    image: nginx:alpine
    container_name: local_mtls_proxy
    ports:
      - "8443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./certs:/etc/nginx/certs:ro # Nginx uses certs from here
    networks:
      - mtls-test-net
    depends_on:
      - litellm-proxy

  litellm-proxy:
    image: ghcr.io/berriai/litellm:main-latest # Consider pinning to a specific stable version later
    container_name: local_litellm_proxy
    expose:
      - "4000"
    ports:
      - "4000:4000"
    volumes:
      - ./litellm_proxy/config.yaml:/app/config.yaml:ro
    command: [ "--config", "/app/config.yaml", "--port", "4000", "--host", "0.0.0.0" ]
    networks:
      - mtls-test-net
    depends_on:
      - ollama # LiteLLM waits for Ollama
    environment: # <-- Add this section
      - LITELLM_MASTER_KEY=sk-1234

  ollama:
    build: ./ollama # Build from the custom Ollama Dockerfile
    container_name: local_ollama
    expose:
      - "11434"
    ports:
      - "11434:11434"
    networks:
      - mtls-test-net
    environment:
      - OLLAMA_PULL_MODEL=qwen3:0.6b # Specify model here, used by entrypoint.sh
      # Set OLLAMA_MODELS to the path where models will be stored inside the container.
      # This should match the volume mount path.
      - OLLAMA_MODELS=/root/.ollama/models
    volumes:
      # Mount the named volume 'ollama_models_cache' to /root/.ollama inside the container.
      # This directory is where Ollama stores its models by default.
      - ollama_models_cache:/root/.ollama
    # deploy: # Optional GPU support
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: 1
    #           capabilities: [gpu]

networks:
  mtls-test-net:
    driver: bridge

# Define the named volume for caching Ollama models
volumes:
  ollama_models_cache:
    driver: local # Specifies the local driver, which is the default
