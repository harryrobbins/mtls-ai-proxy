# tests/test_custom_mtls_stream.py
import pytest
import asyncio
import litellm
from litellm import completion, acompletion
from litellm.types.utils import GenericStreamingChunk, ModelResponse
from pathlib import Path
import os
import traceback
import time
import uuid
import ssl
import httpx
import openai
from openai.types.chat import ChatCompletion
# No need to import these specifically for isinstance checks, the types come from litellm
# from litellm.types.utils import Delta, StreamingChoices, GenericStreamingChunk

# Assuming MTLSOpenAILLM is in the same directory relative to the project root
# Adjust the import path based on the actual structure if necessary
import sys
sys.path.insert(0, str(Path(__file__).parent.parent / "python-client"))

try:
    from custom_mtls_stream_mock_handler import MTLSOpenAILLM
except ImportError:
    pytest.fail("Could not import MTLSOpenAILLM. Ensure custom_mtls_stream_mock_handler.py is in python-client directory.")


# --- Certificate Paths ---
# These should point to the certs generated by the cert-generator service
# Paths are relative to the test file, assuming certs are in the project root's certs dir
BASE_DIR = Path(__file__).resolve().parent.parent
CERTS_DIR = BASE_DIR / "certs"
CLIENT_CERT_PATH = CERTS_DIR / "client.crt"
CLIENT_KEY_PATH = CERTS_DIR / "client.key"
CA_CERT_PATH = CERTS_DIR / "ca.crt"

# --- Configuration ---
# Should match the settings in custom_mtls_stream_mock_handler.py and docker-compose.yml
# Use a different model name for LiteLLM to route requests through the custom handler
TEST_CUSTOM_MODEL_NAME = "mtls-ollama-fake-stream" # This name is used in litellm.completion calls
# The actual model name used by the custom handler to call the upstream LiteLLM proxy
UPSTREAM_MODEL_NAME = "ollama-qwen-local"
LITELLM_PROXY_KEY = "sk-1234"
MTLS_PROXY_URL = "https://localhost:8443" # Nginx mTLS proxy URL

# Validate Certificate Paths - these files are required for the tests
for path_to_check in (CLIENT_CERT_PATH, CLIENT_KEY_PATH, CA_CERT_PATH):
    if not path_to_check.exists():
        pytest.skip(f"Certificate file missing: {path_to_check}. Please run `docker-compose up cert-generator`.")


# --- Fixture to register the custom LLM provider ---
@pytest.fixture(scope="module")
def register_mtls_custom_llm():
    """
    Registers the MTLSOpenAILLM custom LLM provider with LiteLLM.
    """
    # Initialize the custom handler instance
    # The handler instance will load cert paths internally based on its definition
    mtls_custom_llm_instance = MTLSOpenAILLM()

    # Store the original custom_provider_map to restore it after tests
    original_custom_provider_map = litellm.custom_provider_map

    # Register the custom handler
    # Ensure custom_provider_map is a list and append
    if isinstance(litellm.custom_provider_map, list):
        current_map = list(litellm.custom_provider_map)
    elif isinstance(litellm.custom_provider_map, dict): # Handle case where it might be a dict
         current_map = [{"provider": name, "custom_handler": handler} for name, handler in litellm.custom_provider_map.items()]
    else:
        current_map = [] # Default to empty list

    # Add the new custom handler registration
    # Use the TEST_CUSTOM_MODEL_NAME for the provider name to route calls
    current_map.append({"provider": TEST_CUSTOM_MODEL_NAME, "custom_handler": mtls_custom_llm_instance})

    litellm.custom_provider_map = current_map
    print(f"\nRegistered custom LiteLLM provider: '{TEST_CUSTOM_MODEL_NAME}'")
    print(f"Current litellm.custom_provider_map: {litellm.custom_provider_map}")

    # Yield control to the tests
    yield

    # Teardown: Restore the original custom_provider_map
    litellm.custom_provider_map = original_custom_provider_map
    print(f"\nUnregistered custom LiteLLM provider. Restored original map: {litellm.custom_provider_map}")

# Use the fixture for all tests in this file
pytestmark = pytest.mark.usefixtures("register_mtls_custom_llm", "ensure_services_are_up_and_certs_generated")


# --- Test Cases ---

def test_sync_non_streaming():
    """
    Tests synchronous non-streaming completion using the custom mTLS handler.
    Corresponds to the first test in custom_mtls_stream_mock_handler.py's __main__ block.
    """
    print("\nTesting synchronous non-streaming completion...")
    try:
        resp = completion(
            model=f"{TEST_CUSTOM_MODEL_NAME}/sync-non-stream-test", # Use the registered provider name
            messages=[{"role": "user", "content": "Hello from sync non-stream test!"}],
            # Pass optional parameters if needed by the custom handler or upstream
            # max_tokens=10, # Example
            # temperature=0.5, # Example
        )
        print(f"Sync non-stream response received: {resp}")

        assert resp is not None, "Sync non-streaming response is None."
        assert hasattr(resp, 'choices') and len(resp.choices) > 0, "Sync non-streaming response has no choices."
        assert hasattr(resp.choices[0], 'message') and resp.choices[0].message is not None, "Sync non-streaming response message is None."
        assert hasattr(resp.choices[0].message, 'content') and resp.choices[0].message.content is not None, "Sync non-streaming response content is None."
        assert isinstance(resp.choices[0].message.content, str), "Sync non-streaming response content is not a string."
        assert len(resp.choices[0].message.content) > 0, "Sync non-streaming response content is empty."
        print(f"Sync Non-Streaming Content: '{resp.choices[0].message.content}'")

        # Optionally check usage if the handler populates it
        assert hasattr(resp, 'usage') and resp.usage is not None, "Sync non-streaming response usage is None."
        print(f"Sync Non-Streaming Usage: {resp.usage}")

    except Exception as e:
        pytest.fail(f"Synchronous non-streaming test failed: {e}\n{traceback.format_exc()}")


@pytest.mark.asyncio
async def test_async_non_streaming():
    """
    Tests asynchronous non-streaming completion using the custom mTLS handler.
    Corresponds to the second test in custom_mtls_stream_mock_handler.py's __main__ block.
    """
    print("\nTesting asynchronous non-streaming completion...")
    try:
        resp = await acompletion(
            model=f"{TEST_CUSTOM_MODEL_NAME}/async-non-stream-test", # Use the registered provider name
            messages=[{"role": "user", "content": "Hello from async non-stream test!"}],
            # Pass optional parameters if needed
            # max_tokens=10, # Example
            # temperature=0.5, # Example
        )
        print(f"Async non-stream response received: {resp}")

        assert resp is not None, "Async non-streaming response is None."
        assert hasattr(resp, 'choices') and len(resp.choices) > 0, "Async non-streaming response has no choices."
        assert hasattr(resp.choices[0], 'message') and resp.choices[0].message is not None, "Async non-streaming response message is None."
        assert hasattr(resp.choices[0].message, 'content') and resp.choices[0].message.content is not None, "Async non-streaming response content is None."
        assert isinstance(resp.choices[0].message.content, str), "Async non-streaming response content is not a string."
        assert len(resp.choices[0].message.content) > 0, "Async non-streaming response content is empty."
        print(f"Async Non-Streaming Content: '{resp.choices[0].message.content}'")

        # Optionally check usage
        assert hasattr(resp, 'usage') and resp.usage is not None, "Async non-streaming response usage is None."
        print(f"Async Non-Streaming Usage: {resp.usage}")

    except Exception as e:
        pytest.fail(f"Asynchronous non-streaming test failed: {e}\n{traceback.format_exc()}")


def test_sync_streaming():
    """
    Tests synchronous streaming using the custom mTLS handler (which fakes streaming).
    Corresponds to the third test in custom_mtls_stream_mock_handler.py's __main__ block.
    """
    print("\nTesting synchronous streaming...")
    try:
        response_stream = completion(
            model=f"{TEST_CUSTOM_MODEL_NAME}/sync-stream-test", # Use the registered provider name
            messages=[{"role": "user", "content": "Hello from sync stream test! Say this back."}],
            stream=True,
            # max_tokens=15, # Example
            stream_options={"include_usage": True} # Ensure usage is included in the final chunk
        )

        full_response_content_sync = ""
        usage_sync = None
        chunks_received = 0
        last_finish_reason = None

        # Iterate over the stream
        for chunk in response_stream:
            print(f"Received sync stream chunk: {chunk}")
            chunks_received += 1

            # Check if it's a ModelResponseStream (the wrapper from LiteLLM)
            assert isinstance(chunk, litellm.ModelResponseStream), f"Expected ModelResponseStream, got {type(chunk)}"

            # Extract content and accumulate
            if chunk.choices and len(chunk.choices) > 0 and chunk.choices[0].delta and chunk.choices[0].delta.content:
                 content = chunk.choices[0].delta.content
                 full_response_content_sync += content
            else:
                 content = None # No new content in this chunk

            # Extract finish reason if present
            if chunk.choices and len(chunk.choices) > 0 and chunk.choices[0].finish_reason is not None:
                 last_finish_reason = chunk.choices[0].finish_reason

            # Capture usage if present on the chunk (can be on the final chunk)
            if hasattr(chunk, 'usage') and chunk.usage is not None:
                 usage_sync = chunk.usage


            print(f"  Sync Stream Chunk {chunks_received}: finish_reason='{last_finish_reason}', content='{content or ''}'")


        print(f"Full Sync Streamed Response: '{full_response_content_sync}'")
        print(f"Sync Stream Usage: {usage_sync}")

        # Assertions
        assert chunks_received > 0, "No chunks received for sync streaming."
        assert len(full_response_content_sync) > 0, "Sync streamed content is empty."
        assert last_finish_reason is not None, "Sync streaming did not receive a final chunk with a finish_reason."
        assert usage_sync is not None, "Sync streaming usage is None (stream_options={'include_usage': True} might not be supported or usage not populated by handler)."
        # Check usage values are plausible if usage_sync is not None
        if usage_sync:
             assert usage_sync.get("total_tokens", 0) > 0, "Sync streaming usage has zero total_tokens."


    except Exception as e:
        pytest.fail(f"Synchronous streaming test failed: {e}\n{traceback.format_exc()}")


@pytest.mark.asyncio
async def test_async_streaming():
    """
    Tests asynchronous streaming using the custom mTLS handler (which fakes streaming).
    Corresponds to the fourth test in custom_mtls_stream_mock_handler.py's __main__ block.
    """
    print("\nTesting asynchronous streaming...")
    try:
        response_stream = await acompletion(
            model=f"{TEST_CUSTOM_MODEL_NAME}/async-stream-test", # Use the registered provider name
            messages=[{"role": "user", "content": "Hello from async stream test! Say this back."}],
            stream=True,
            # max_tokens=15, # Example
            stream_options={"include_usage": True} # Ensure usage is included in the final chunk
        )

        full_response_content_async = ""
        usage_async = None
        chunks_received = 0
        last_finish_reason = None

        # Iterate over the async stream
        async for chunk in response_stream:
            print(f"Received async stream chunk: {chunk}")
            chunks_received += 1

            # Check if it's a ModelResponseStream (the wrapper from LiteLLM)
            assert isinstance(chunk, litellm.ModelResponseStream), f"Expected ModelResponseStream, got {type(chunk)}"

            # Extract content and accumulate
            if chunk.choices and len(chunk.choices) > 0 and chunk.choices[0].delta and chunk.choices[0].delta.content:
                 content = chunk.choices[0].delta.content
                 full_response_content_async += content
            else:
                 content = None # No new content in this chunk

            # Extract finish reason if present
            if chunk.choices and len(chunk.choices) > 0 and chunk.choices[0].finish_reason is not None:
                 last_finish_reason = chunk.choices[0].finish_reason

            # Capture usage if present on the chunk (can be on the final chunk)
            if hasattr(chunk, 'usage') and chunk.usage is not None:
                 usage_async = chunk.usage


            print(f"  Async Stream Chunk {chunks_received}: finish_reason='{last_finish_reason}', content='{content or ''}'")


        print(f"Full Async Streamed Response: '{full_response_content_async}'")
        print(f"Async Stream Usage: {usage_async}")


        # Assertions
        assert chunks_received > 0, "No chunks received for async streaming."
        assert len(full_response_content_async) > 0, "Async streamed content is empty."
        assert last_finish_reason is not None, "Async streaming did not receive a final chunk with a finish_reason."
        assert usage_async is not None, "Async streaming usage is None (stream_options={'include_usage': True} might not be supported or usage not populated by handler)."
        # Check usage values are plausible if usage_async is not None
        if usage_async:
             assert usage_async.get("total_tokens", 0) > 0, "Async streaming usage has zero total_tokens."


    except Exception as e:
        pytest.fail(f"Asynchronous streaming test failed: {e}\n{traceback.format_exc()}")